{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4062b37c-c7b6-4a4a-88b7-b8e649e8eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(parent_dir, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from loss import OrdinalLoss\n",
    "from utils import compute_graded_metrics\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "050a9684-696d-4d08-89e4-2e8377eb60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU: NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LEN = 512   # tokens\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_CLASSES = 4\n",
    "ALPHA_ORDINAL = 1.5\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")   # torch.device(\"cuda:0\")\n",
    "print(\"Using CUDA GPU:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "DATA_DIR = '../data/processed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4f7ddbe-e96c-4862-a83d-39b53c2c740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 11972\n",
      "Val size: 1605\n",
      "Test size: 1036\n",
      "      users                                               text  sentiment  \\\n",
      "0        24  If there is a god it’s not the one that you th...  Indicator   \n",
      "1        24  I just wish it was ok to end our lives if that...   Ideation   \n",
      "2        24  Sometimes suicide is a very appropriate reacti...   Ideation   \n",
      "3        24  I’m exhausted but I can’t fall asleep the ment...   Ideation   \n",
      "4        24  The pain just gets worse. How does it keep get...   Ideation   \n",
      "...     ...                                                ...        ...   \n",
      "1031   1235  I want to kill myself but I'm scared. And I do...   Ideation   \n",
      "1032   1253  Yeah, that is how it is sometimes. I'll be gon...   Behavior   \n",
      "1033   1253  haha, noone's going to see this but here goes ...   Behavior   \n",
      "1034   1259  Just venting I tried to kill myself about two ...    Attempt   \n",
      "1035   1261  I did it.. I made a reddit account. Purely to ...  Indicator   \n",
      "\n",
      "            time        timestamp_dt  label_ordinal  \n",
      "0     1602823395 2020-10-16 04:43:15              0  \n",
      "1     1602826506 2020-10-16 05:35:06              1  \n",
      "2     1602901712 2020-10-17 02:28:32              1  \n",
      "3     1602924825 2020-10-17 08:53:45              1  \n",
      "4     1602926864 2020-10-17 09:27:44              1  \n",
      "...          ...                 ...            ...  \n",
      "1031  1603197380 2020-10-20 12:36:20              1  \n",
      "1032  1608418348 2020-12-19 22:52:28              2  \n",
      "1033  1609926971 2021-01-06 09:56:11              2  \n",
      "1034  1603464843 2020-10-23 14:54:03              3  \n",
      "1035  1607733133 2020-12-12 00:32:13              0  \n",
      "\n",
      "[1036 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch-nvme/3800433/ipykernel_1969175/4138350335.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  train_df = pickle.load(open(\"../data/processed/train.pkl\", \"rb\"))\n",
      "/localscratch-nvme/3800433/ipykernel_1969175/4138350335.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  val_df   = pickle.load(open(\"../data/processed/val.pkl\", \"rb\"))\n",
      "/localscratch-nvme/3800433/ipykernel_1969175/4138350335.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  test_df  = pickle.load(open(\"../data/processed/test.pkl\", \"rb\"))\n"
     ]
    }
   ],
   "source": [
    "# Load PKL datasets\n",
    "\n",
    "train_df = pickle.load(open(\"../data/processed/train.pkl\", \"rb\"))\n",
    "val_df   = pickle.load(open(\"../data/processed/val.pkl\", \"rb\"))\n",
    "test_df  = pickle.load(open(\"../data/processed/test.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63238a24-f2f4-455c-8e59-6e336d24800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      users                                               text  sentiment  \\\n",
      "0        24  If there is a god it’s not the one that you th...  Indicator   \n",
      "1        24  I just wish it was ok to end our lives if that...   Ideation   \n",
      "2        24  Sometimes suicide is a very appropriate reacti...   Ideation   \n",
      "3        24  I’m exhausted but I can’t fall asleep the ment...   Ideation   \n",
      "4        24  The pain just gets worse. How does it keep get...   Ideation   \n",
      "...     ...                                                ...        ...   \n",
      "1031   1235  I want to kill myself but I'm scared. And I do...   Ideation   \n",
      "1032   1253  Yeah, that is how it is sometimes. I'll be gon...   Behavior   \n",
      "1033   1253  haha, noone's going to see this but here goes ...   Behavior   \n",
      "1034   1259  Just venting I tried to kill myself about two ...    Attempt   \n",
      "1035   1261  I did it.. I made a reddit account. Purely to ...  Indicator   \n",
      "\n",
      "            time        timestamp_dt  label_ordinal  hour  day_of_week  \\\n",
      "0     1602823395 2020-10-16 04:43:15              0     4            4   \n",
      "1     1602826506 2020-10-16 05:35:06              1     5            4   \n",
      "2     1602901712 2020-10-17 02:28:32              1     2            5   \n",
      "3     1602924825 2020-10-17 08:53:45              1     8            5   \n",
      "4     1602926864 2020-10-17 09:27:44              1     9            5   \n",
      "...          ...                 ...            ...   ...          ...   \n",
      "1031  1603197380 2020-10-20 12:36:20              1    12            1   \n",
      "1032  1608418348 2020-12-19 22:52:28              2    22            5   \n",
      "1033  1609926971 2021-01-06 09:56:11              2     9            2   \n",
      "1034  1603464843 2020-10-23 14:54:03              3    14            4   \n",
      "1035  1607733133 2020-12-12 00:32:13              0     0            5   \n",
      "\n",
      "       time_gap  time_since_start  normalized_gap  \n",
      "0           0.0               0.0        0.000000  \n",
      "1        3111.0            3111.0        8.043021  \n",
      "2       75206.0           78317.0       11.228000  \n",
      "3       23113.0          101430.0       10.048194  \n",
      "4        2039.0          103469.0        7.620705  \n",
      "...         ...               ...             ...  \n",
      "1031        0.0               0.0        0.000000  \n",
      "1032        0.0               0.0        0.000000  \n",
      "1033  1508623.0         1508623.0       14.226709  \n",
      "1034        0.0               0.0        0.000000  \n",
      "1035        0.0               0.0        0.000000  \n",
      "\n",
      "[1036 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "def add_timestamp_features(df):\n",
    "\n",
    "    df[\"timestamp_dt\"] = pd.to_datetime(df[\"timestamp_dt\"])\n",
    "    df[\"hour\"] = df[\"timestamp_dt\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"timestamp_dt\"].dt.dayofweek\n",
    "\n",
    "    # Sort by user & time\n",
    "    df = df.sort_values([\"users\", \"timestamp_dt\"])\n",
    "\n",
    "    # === Generated by Gemini ===\n",
    "    # Time difference to previous post\n",
    "    df[\"time_gap\"] = df.groupby(\"users\")[\"timestamp_dt\"].diff().dt.total_seconds()\n",
    "    df[\"time_gap\"] = df[\"time_gap\"].fillna(0)\n",
    "\n",
    "    # Time since user's first post\n",
    "    df[\"time_since_start\"] = (\n",
    "        df[\"timestamp_dt\"] - df.groupby(\"users\")[\"timestamp_dt\"].transform(\"min\")\n",
    "    ).dt.total_seconds()\n",
    "    # === End of Gemini-generated block ===\n",
    "    \n",
    "    df[\"normalized_gap\"] = np.log1p(df[\"time_gap\"])  # log(1 + seconds)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_timestamp_features(train_df)\n",
    "val_df   = add_timestamp_features(val_df)\n",
    "test_df  = add_timestamp_features(test_df)\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d99dff9-e65a-4a9a-9148-858b4d9f5842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      users                                               text  sentiment  \\\n",
      "0        24  If there is a god it’s not the one that you th...  Indicator   \n",
      "1        24  I just wish it was ok to end our lives if that...   Ideation   \n",
      "2        24  Sometimes suicide is a very appropriate reacti...   Ideation   \n",
      "3        24  I’m exhausted but I can’t fall asleep the ment...   Ideation   \n",
      "4        24  The pain just gets worse. How does it keep get...   Ideation   \n",
      "...     ...                                                ...        ...   \n",
      "1031   1235  I want to kill myself but I'm scared. And I do...   Ideation   \n",
      "1032   1253  Yeah, that is how it is sometimes. I'll be gon...   Behavior   \n",
      "1033   1253  haha, noone's going to see this but here goes ...   Behavior   \n",
      "1034   1259  Just venting I tried to kill myself about two ...    Attempt   \n",
      "1035   1261  I did it.. I made a reddit account. Purely to ...  Indicator   \n",
      "\n",
      "            time        timestamp_dt  label_ordinal  hour  day_of_week  \\\n",
      "0     1602823395 2020-10-16 04:43:15              0     4            4   \n",
      "1     1602826506 2020-10-16 05:35:06              1     5            4   \n",
      "2     1602901712 2020-10-17 02:28:32              1     2            5   \n",
      "3     1602924825 2020-10-17 08:53:45              1     8            5   \n",
      "4     1602926864 2020-10-17 09:27:44              1     9            5   \n",
      "...          ...                 ...            ...   ...          ...   \n",
      "1031  1603197380 2020-10-20 12:36:20              1    12            1   \n",
      "1032  1608418348 2020-12-19 22:52:28              2    22            5   \n",
      "1033  1609926971 2021-01-06 09:56:11              2     9            2   \n",
      "1034  1603464843 2020-10-23 14:54:03              3    14            4   \n",
      "1035  1607733133 2020-12-12 00:32:13              0     0            5   \n",
      "\n",
      "       time_gap  time_since_start  normalized_gap  \n",
      "0           0.0               0.0        0.000000  \n",
      "1        3111.0            3111.0        8.043021  \n",
      "2       75206.0           78317.0       11.228000  \n",
      "3       23113.0          101430.0       10.048194  \n",
      "4        2039.0          103469.0        7.620705  \n",
      "...         ...               ...             ...  \n",
      "1031        0.0               0.0        0.000000  \n",
      "1032        0.0               0.0        0.000000  \n",
      "1033  1508623.0         1508623.0       14.226709  \n",
      "1034        0.0               0.0        0.000000  \n",
      "1035        0.0               0.0        0.000000  \n",
      "\n",
      "[1036 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# DeBERTa Tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "train_encodings = tokenizer(train_df[\"text\"].tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "val_encodings   = tokenizer(val_df[\"text\"].tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "test_encodings  = tokenizer(test_df[\"text\"].tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0633c945-624d-45c0-bc23-a0f4bd2633ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created with BATCH_SIZE= 16\n"
     ]
    }
   ],
   "source": [
    "class TimeAwareDataset(Dataset):\n",
    "    def __init__(self, encodings, df):\n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attention_mask = encodings['attention_mask']\n",
    "        self.targets = torch.tensor(df[\"label_ordinal\"].values, dtype=torch.long)\n",
    "        \n",
    "        # Extract and prepare time features: hour, day of week, time gap, normalized gap\n",
    "        # Ensure the order is consistent with the model's expectations\n",
    "        time_feats = df[[\"hour\", \"day_of_week\", \"normalized_gap\"]].values\n",
    "        self.time_feats = torch.tensor(time_feats, dtype=torch.float32)\n",
    "\n",
    "    # Will be used when calling DataLoader   \n",
    "    def __len__(self): \n",
    "        return len(self.targets)\n",
    "\n",
    "    # Will be used when calling DataLoader\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'time_feats': self.time_feats[idx],\n",
    "            'targets': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Create Datasets & DataLoaders\n",
    "train_dataset = TimeAwareDataset(train_encodings, train_df)\n",
    "val_dataset = TimeAwareDataset(val_encodings, val_df)\n",
    "test_dataset = TimeAwareDataset(test_encodings, test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created with BATCH_SIZE=\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c9302-b7f6-4207-b74f-af646669454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, Loss, and Optimizer initialized.\n"
     ]
    }
   ],
   "source": [
    "class TimeAwareOrdinalModel(nn.Module):\n",
    "    def __init__(self, model_name, time_feat_dim, num_classes, lstm_hidden_size=256, dropout_rate=0.3):\n",
    "        super(TimeAwareOrdinalModel, self).__init__()\n",
    "        \n",
    "        # 1. DeBERTa Pre-trained model\n",
    "        self.text_model = AutoModel.from_pretrained(model_name)\n",
    "        embed_dim = self.text_model.config.hidden_size # default DeBERTa Base is 768\n",
    "        \n",
    "        # 2. BiLSTM layer: Used to process DeBERTa sequence output\n",
    "        # Input size is 768 (DeBERTa hidden layer size)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=lstm_hidden_size, # 256\n",
    "            num_layers=1,\n",
    "            bidirectional=True, # bi-direction\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # BiLSTM output dimension: 2 * lstm_hidden_size (because it is bidirectional)\n",
    "        bilstm_output_dim = lstm_hidden_size * 2\n",
    "        \n",
    "        # 3. Total input dimension of merged features\n",
    "        # Total input dimension = BiLSTM output + temporal features (3)\n",
    "        total_input_dim = bilstm_output_dim + time_feat_dim\n",
    "        \n",
    "        # 4. Classifier header\n",
    "        self.fc1 = nn.Linear(total_input_dim, lstm_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(lstm_hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, time_feats):\n",
    "        \n",
    "        # 1. Get DeBERTa sequence output\n",
    "        # text_output.last_hidden_state shape: (B, MAX_LEN, embed_dim=768)\n",
    "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = text_output.last_hidden_state\n",
    "        \n",
    "        # 2. Passing to BiLSTM\n",
    "        # Inputting the sequence into BiLSTM\n",
    "        lstm_output, _ = self.bilstm(sequence_output)\n",
    "        \n",
    "        # 3. Extracting the output of a BiLSTM: Typically, we take the output of the last time step of the sequence (or the sequence after max pooling/average pooling).\n",
    "        # Here we take the output of the last time step (the last token) of the BiLSTM.\n",
    "        # lstm_output shape: (B, MAX_LEN, 2 * lstm_hidden_size)\n",
    "        lstm_pooled = lstm_output[:, 0, :] # Extract the BiLSTM output of the first token (similar to [CLS]) from the sequence.\n",
    "        \n",
    "        # 4. Combined features: [BiLSTM output, temporal features] \n",
    "        combined_features = torch.cat((lstm_pooled, time_feats), dim=1)\n",
    "        \n",
    "        # 5. Passed to classification layer\n",
    "        out = self.fc1(combined_features)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.output_layer(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Instantiation of model and loss function\n",
    "TIME_FEAT_DIM = 3 # hour, day_of_week, normalized_gap\n",
    "model = TimeAwareOrdinalModel(MODEL_NAME, TIME_FEAT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "loss_fn = OrdinalLoss(alpha=ALPHA_ORDINAL, num_classes=NUM_CLASSES, device=DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model, Loss, and Optimizer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2f512f4-a7eb-4833-ac33-308baebc7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        time_feats = batch['time_feats'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, time_feats)\n",
    "        \n",
    "        # OrdinalLoss returns a shape of (B,) which needs to be calculated using mean().\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.mean().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d065234-2282-4b12-a976-4b7559c4cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            time_feats = batch['time_feats'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, time_feats)\n",
    "            \n",
    "            # Predicted category (index with the highest logit count)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "            metrics = compute_graded_metrics(all_targets, all_preds)\n",
    "    return metrics\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5fd7b0b-8326-4421-b8c6-11142f24aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 749/749 [05:55<00:00,  2.11it/s]\n",
      "Evaluating: 100%|██████████| 101/101 [00:16<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1327\n",
      "Validation Metrics: Graded Precision=0.8137, Graded Recall=0.8879, Graded F1=0.8492\n",
      "Model saved! New best F1.\n",
      "\n",
      "--- Epoch 2/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 749/749 [05:55<00:00,  2.11it/s]\n",
      "Evaluating: 100%|██████████| 101/101 [00:16<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0516\n",
      "Validation Metrics: Graded Precision=0.8511, Graded Recall=0.8928, Graded F1=0.8715\n",
      "Model saved! New best F1.\n",
      "\n",
      "--- Epoch 3/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 749/749 [05:55<00:00,  2.11it/s]\n",
      "Evaluating: 100%|██████████| 101/101 [00:16<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0121\n",
      "Validation Metrics: Graded Precision=0.8953, Graded Recall=0.8517, Graded F1=0.873\n",
      "Model saved! New best F1.\n",
      "\n",
      "--- Epoch 4/4 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 749/749 [05:55<00:00,  2.11it/s]\n",
      "Evaluating: 100%|██████████| 101/101 [00:16<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9830\n",
      "Validation Metrics: Graded Precision=0.8548, Graded Recall=0.8941, Graded F1=0.874\n",
      "Model saved! New best F1.\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "MODEL_SAVE_PATH = \"time_aware_deberta_ordinal_best.pt\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    \n",
    "    # Train\n",
    "    avg_train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, DEVICE)\n",
    "    \n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Metrics: Graded Precision={val_metrics['graded_precision']}, Graded Recall={val_metrics['graded_recall']}, Graded F1={val_metrics['graded_f1']}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_metrics['graded_f1'] > best_f1:\n",
    "        best_f1 = val_metrics['graded_f1']\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(\"Model saved! New best F1.\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67c62211-d58a-4225-ad35-4a5ed71be80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Set Evaluation ---\n",
      "Loaded best model from time_aware_deberta_ordinal_best.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 65/65 [00:10<00:00,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n",
      "Graded Precision: 0.8417\n",
      "Graded Recall: 0.8658\n",
      "Graded F1 Score: 0.8536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Test Set Evaluation ---\")\n",
    "\n",
    "# Load the best model\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    print(f\"Loaded best model from {MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Best model file {MODEL_SAVE_PATH} not found. Using final epoch model.\")\n",
    "\n",
    "# Evaluate test set\n",
    "test_metrics = evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Graded Precision: {test_metrics['graded_precision']}\")\n",
    "print(f\"Graded Recall: {test_metrics['graded_recall']}\")\n",
    "print(f\"Graded F1 Score: {test_metrics['graded_f1']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4aa6222f-07ae-4d33-9fe0-825b1d56ea86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Predicting on Test Set ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting: 100%|██████████| 65/65 [00:10<00:00,  6.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Accuracy: 0.7075\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Indicator       0.77      0.67      0.71       305\n",
      "    Ideation       0.73      0.80      0.76       530\n",
      "    Behavior       0.54      0.56      0.55       135\n",
      "     Attempt       0.62      0.47      0.53        66\n",
      "\n",
      "    accuracy                           0.71      1036\n",
      "   macro avg       0.66      0.62      0.64      1036\n",
      "weighted avg       0.71      0.71      0.71      1036\n",
      "\n",
      "\n",
      "=== Graded Metrics ===\n",
      "Graded Precision: 0.8417\n",
      "Graded Recall:    0.8658\n",
      "Graded F1-Score:  0.8536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load the best saved model\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH)) # Ensure path matches your save\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"--- Predicting on Test Set ---\")\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        # 1. Load data to device\n",
    "        input_ids = d[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = d[\"attention_mask\"].to(DEVICE)\n",
    "        # Fix: You must extract time_feats as your model requires them\n",
    "        time_feats = d[\"time_feats\"].to(DEVICE) \n",
    "        \n",
    "        # Fix: Your Dataset defined this key as 'targets', not 'labels'\n",
    "        targets = d[\"targets\"].to(DEVICE)\n",
    "\n",
    "        # 2. Forward Pass\n",
    "        # Fix: Pass time_feats to the model\n",
    "        logits = model(input_ids, attention_mask, time_feats)\n",
    "        \n",
    "        # 3. Get Predictions\n",
    "        # Fix: Your model returns raw logits directly, not an object with .logits attribute\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        \n",
    "        y_pred_list.extend(preds.cpu().numpy())\n",
    "        y_true_list.extend(targets.cpu().numpy())\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "\n",
    "# 1. Simple Accuracy\n",
    "acc = accuracy_score(y_true_list, y_pred_list)\n",
    "print(f\"\\nSimple Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 2. Detailed Report\n",
    "target_names = ['Indicator', 'Ideation', 'Behavior', 'Attempt']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_list, y_pred_list, target_names=target_names))\n",
    "\n",
    "# 3. Graded Metrics (Using your custom util)\n",
    "graded_metrics = compute_graded_metrics(y_true_list, y_pred_list)\n",
    "print(\"\\n=== Graded Metrics ===\")\n",
    "print(f\"Graded Precision: {graded_metrics['graded_precision']:.4f}\")\n",
    "print(f\"Graded Recall:    {graded_metrics['graded_recall']:.4f}\")\n",
    "print(f\"Graded F1-Score:  {graded_metrics['graded_f1']:.4f}\")\n",
    "\n",
    "# ===== END: Gemini-generated block ====="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3fdfc-f893-4438-a807-6148af230d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv project)",
   "language": "python",
   "name": "project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
