{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d50b5e3-ec69-4924-8cbd-c2c4a99aecc3",
   "metadata": {},
   "source": [
    "# **Approach 1: Machine Learning Baseline (XGBoost)**\n",
    "\n",
    "### **Overview**\n",
    "This notebook implements the machine learning baseline for suicide risk detection. We use **XGBoost**, a powerful gradient boosting algorithm, trained on a combination of:\n",
    "1.  **TF-IDF Vectors:** To capture n-gram textual patterns.\n",
    "2.  **Psycholinguistic Features:** To capture emotional tone and semantic topics.\n",
    "\n",
    "### **Feature Engineering Strategy**\n",
    "Instead of using the proprietary **LIWC** software, we adopt a robust open-source framework based on recent literature:\n",
    "* **Empath:** Used to extract semantic categories (e.g., 'death', 'pain', 'family'). Empath has been validated to correlate highly ($r > 0.9$) with LIWC categories.\n",
    "* **TextBlob:** Used to extract **Sentiment Polarity** and **Subjectivity**, following the forensic text analysis methodology of Adkins et al. (2025).\n",
    "\n",
    "This approach ensures reproducibility and transparency while maintaining high feature quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404ac24-0628-4ad0-a325-33330366cced",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Imports & Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ec6f03-8ae9-432b-9a36-00e302f05ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import scipy.sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from textblob import TextBlob\n",
    "from empath import Empath\n",
    "import xgboost as xgb\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.utils import compute_graded_metrics\n",
    "\n",
    "PROCESSED_DATA_DIR = '../data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9feefc-be83-47f5-9a5d-0ff624bd78a5",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceb23de8-6d5e-49f2-affb-d2b38d26921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Val, Test size: 11972, 1605, 1036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>time</th>\n",
       "      <th>timestamp_dt</th>\n",
       "      <th>label_ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No one understands how much I desperately want...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>1648483701</td>\n",
       "      <td>2022-03-28 16:08:21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Today I never wanted to live to see 25. That m...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>1651130449</td>\n",
       "      <td>2022-04-28 07:20:49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Suicidal thoughts at / because of school For s...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>1662712545</td>\n",
       "      <td>2022-09-09 08:35:45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I feel like the pain will never end Everyday f...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>1638628371</td>\n",
       "      <td>2021-12-04 14:32:51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Is there even a point to living if you're not ...</td>\n",
       "      <td>Indicator</td>\n",
       "      <td>1639749228</td>\n",
       "      <td>2021-12-17 13:53:48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users                                               text  sentiment  \\\n",
       "0      1  No one understands how much I desperately want...   Ideation   \n",
       "1      2  Today I never wanted to live to see 25. That m...   Behavior   \n",
       "2      3  Suicidal thoughts at / because of school For s...   Ideation   \n",
       "3      4  I feel like the pain will never end Everyday f...   Ideation   \n",
       "4      4  Is there even a point to living if you're not ...  Indicator   \n",
       "\n",
       "         time        timestamp_dt  label_ordinal  \n",
       "0  1648483701 2022-03-28 16:08:21              1  \n",
       "1  1651130449 2022-04-28 07:20:49              2  \n",
       "2  1662712545 2022-09-09 08:35:45              1  \n",
       "3  1638628371 2021-12-04 14:32:51              1  \n",
       "4  1639749228 2021-12-17 13:53:48              0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'train.pkl'))\n",
    "val_df = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'val.pkl'))\n",
    "test_df = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'test.pkl'))\n",
    "\n",
    "print(f\"Train, Val, Test size: {len(train_df)}, {len(val_df)}, {len(test_df)}\")\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f929bf5-ad5c-4765-8918-1fd80e4e9c45",
   "metadata": {},
   "source": [
    "---\n",
    "### **Feature Engineering 1: Psycholinguistic Feature Extraction**\n",
    "\n",
    "We implement a feature extractor combining **TextBlob** and **Empath**.\n",
    "\n",
    "**References:**\n",
    "* **Fast, E., Chen, B., & Bernstein, M. S. (2016).** *Empath: Understanding Topic Signals in Large-Scale Text.* CHI 2016. (Validated Empath against LIWC).\n",
    "* **Adkins, J., Al Bataineh, A., & Khanal, A. (2025).** *A psycholinguistic NLP framework for forensic text analysis of deception and emotion.* Frontiers in AI. (Used TextBlob for sentiment/subjectivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a47354c-8446-46b6-94f4-befcdc8400bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Train Set ---\n",
      "Extracting features for 11972 posts...\n",
      "--- Processing Val Set ---\n",
      "Extracting features for 1605 posts...\n",
      "--- Processing Test Set ---\n",
      "Extracting features for 1036 posts...\n",
      "Psycholinguistic Feature Shape: (11972, 14)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Empath lexicon\n",
    "lexicon = Empath()\n",
    "\n",
    "# ===== BEGIN: Gemini-generated block =====\n",
    "def get_psycholinguistic_features(texts):\n",
    "    \"\"\"\n",
    "    Extracts psycholinguistic features to replace LIWC.\n",
    "    \n",
    "    Features include:\n",
    "    1. Basic Stats: Word count, First-person pronoun ratio (I-usage).\n",
    "    2. Sentiment & Subjectivity: Calculated via TextBlob (Adkins et al., 2025).\n",
    "    3. Semantic Topics: Calculated via Empath (Fast et al., 2016).\n",
    "    \"\"\"\n",
    "    # Define specific categories relevant to suicide risk detection\n",
    "    # These align with standard LIWC categories often used in mental health research\n",
    "    target_categories = [\n",
    "        \"death\", \"pain\", \"medical\", \"negative_emotion\", \n",
    "        \"sadness\", \"anxiety\", \"family\", \"friend\", \"work\", \"swearing\"\n",
    "    ]\n",
    "    \n",
    "    features = []\n",
    "    print(f\"Extracting features for {len(texts)} posts...\")\n",
    "    \n",
    "    for text in texts:\n",
    "        text_str = str(text)\n",
    "        blob = TextBlob(text_str)\n",
    "        words = text_str.lower().split()\n",
    "        num_words = max(1, len(words)) # Avoid division by zero\n",
    "        \n",
    "        # --- 1. Basic Linguistic Statistics ---\n",
    "        # Self-references (I, me, my) are strong indicators of self-focus in depression\n",
    "        i_count = sum(1 for w in words if w in ['i', 'me', 'my', 'mine', 'myself'])\n",
    "        i_ratio = i_count / num_words\n",
    "        \n",
    "        # --- 2. TextBlob Features (Adkins et al., 2025) ---\n",
    "        # Polarity: -1.0 (Negative) to 1.0 (Positive)\n",
    "        # Subjectivity: 0.0 (Objective) to 1.0 (Subjective)\n",
    "        polarity = blob.sentiment.polarity\n",
    "        subjectivity = blob.sentiment.subjectivity\n",
    "        \n",
    "        # --- 3. Empath Features (Fast et al., 2016) ---\n",
    "        # Analyze text against the target categories\n",
    "        # normalize=True divides the count by total words (similar to LIWC)\n",
    "        empath_scores = lexicon.analyze(text_str.lower(), categories=target_categories, normalize=True)\n",
    "        \n",
    "        # Handle cases where Empath might return None for empty strings\n",
    "        if not empath_scores:\n",
    "            empath_scores = {cat: 0.0 for cat in target_categories}\n",
    "            \n",
    "        # Combine all features into a single row\n",
    "        row = [num_words, i_ratio, polarity, subjectivity] + [empath_scores[cat] for cat in target_categories]\n",
    "        features.append(row)\n",
    "        \n",
    "    return np.array(features)\n",
    "\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "# Apply extraction to all splits\n",
    "# (Note: This might take a minute or two depending on dataset size)\n",
    "print(\"--- Processing Train Set ---\")\n",
    "X_train_psych = get_psycholinguistic_features(train_df['text'])\n",
    "\n",
    "print(\"--- Processing Val Set ---\")\n",
    "X_val_psych = get_psycholinguistic_features(val_df['text'])\n",
    "\n",
    "print(\"--- Processing Test Set ---\")\n",
    "X_test_psych = get_psycholinguistic_features(test_df['text'])\n",
    "\n",
    "print(f\"Psycholinguistic Feature Shape: {X_train_psych.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0d9e9-9fde-42cd-92f5-bf834221aa00",
   "metadata": {},
   "source": [
    "### **Feature Engineering 2: TF-IDF (N-grams)**\n",
    "*default: lowercase=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b6991b0-0805-4c16-93ff-4cc238d00dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Train Shape: (11972, 5000)\n",
      "TF-IDF Val Shape:   (1605, 5000)\n",
      "TF-IDF Test Shape:  (1036, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Limit features to top 5000 to prevent overfitting and high dimensionality\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "\n",
    "# Fit on TRAIN only to prevent data leakage\n",
    "X_train_tfidf = tfidf.fit_transform(train_df['text'])\n",
    "X_val_tfidf = tfidf.transform(val_df['text'])\n",
    "X_test_tfidf = tfidf.transform(test_df['text'])\n",
    "\n",
    "print(f\"TF-IDF Train Shape: {X_train_tfidf.shape}\")\n",
    "print(f\"TF-IDF Val Shape:   {X_val_tfidf.shape}\")\n",
    "print(f\"TF-IDF Test Shape:  {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426cc7eb-9fe2-48a9-991d-a9a76afa3ffc",
   "metadata": {},
   "source": [
    "### **Feature Combination**\n",
    "\n",
    "1.  **Dense Matrix:** Psycholinguistic features.\n",
    "2.  **Sparse Matrix:** TF-IDF vectors.\n",
    "\n",
    "These are horizontally stacked to form the final training input ($X$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adaabcbb-cf7a-4d64-ad04-cea1042a24fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Training Data Shape:   (11972, 5014)\n",
      "Final Validation Data Shape: (1605, 5014)\n",
      "Final Test Data Shape:       (1036, 5014)\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "# Stack TF-IDF (Sparse) with Psycholinguistic (Dense) matrices\n",
    "X_train = sp.hstack([X_train_tfidf, X_train_psych])\n",
    "X_val = sp.hstack([X_val_tfidf, X_val_psych])\n",
    "X_test = sp.hstack([X_test_tfidf, X_test_psych])\n",
    "\n",
    "# Prepare target labels (to Numpy array)\n",
    "y_train = train_df['label_ordinal'].values\n",
    "y_val = val_df['label_ordinal'].values\n",
    "y_test = test_df['label_ordinal'].values\n",
    "\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "print(f\"Final Training Data Shape:   {X_train.shape}\")\n",
    "print(f\"Final Validation Data Shape: {X_val.shape}\")\n",
    "print(f\"Final Test Data Shape:       {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9224e7d-b21e-4596-87d2-289b4b3e1df8",
   "metadata": {},
   "source": [
    "### **Model Training (XGBoost)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54ce6a10-918d-4ab4-bf9d-f75dec486c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start\n",
      "[0]\tvalidation_0-mlogloss:1.22856\n",
      "[1]\tvalidation_0-mlogloss:1.18457\n",
      "[2]\tvalidation_0-mlogloss:1.14708\n",
      "[3]\tvalidation_0-mlogloss:1.11378\n",
      "[4]\tvalidation_0-mlogloss:1.08502\n",
      "[5]\tvalidation_0-mlogloss:1.05914\n",
      "[6]\tvalidation_0-mlogloss:1.03659\n",
      "[7]\tvalidation_0-mlogloss:1.01733\n",
      "[8]\tvalidation_0-mlogloss:0.99862\n",
      "[9]\tvalidation_0-mlogloss:0.98234\n",
      "[10]\tvalidation_0-mlogloss:0.96882\n",
      "[11]\tvalidation_0-mlogloss:0.95544\n",
      "[12]\tvalidation_0-mlogloss:0.94358\n",
      "[13]\tvalidation_0-mlogloss:0.93368\n",
      "[14]\tvalidation_0-mlogloss:0.92363\n",
      "[15]\tvalidation_0-mlogloss:0.91556\n",
      "[16]\tvalidation_0-mlogloss:0.90859\n",
      "[17]\tvalidation_0-mlogloss:0.90113\n",
      "[18]\tvalidation_0-mlogloss:0.89451\n",
      "[19]\tvalidation_0-mlogloss:0.88790\n",
      "[20]\tvalidation_0-mlogloss:0.88208\n",
      "[21]\tvalidation_0-mlogloss:0.87708\n",
      "[22]\tvalidation_0-mlogloss:0.87280\n",
      "[23]\tvalidation_0-mlogloss:0.86835\n",
      "[24]\tvalidation_0-mlogloss:0.86363\n",
      "[25]\tvalidation_0-mlogloss:0.86019\n",
      "[26]\tvalidation_0-mlogloss:0.85640\n",
      "[27]\tvalidation_0-mlogloss:0.85367\n",
      "[28]\tvalidation_0-mlogloss:0.85031\n",
      "[29]\tvalidation_0-mlogloss:0.84703\n",
      "[30]\tvalidation_0-mlogloss:0.84429\n",
      "[31]\tvalidation_0-mlogloss:0.84171\n",
      "[32]\tvalidation_0-mlogloss:0.83908\n",
      "[33]\tvalidation_0-mlogloss:0.83689\n",
      "[34]\tvalidation_0-mlogloss:0.83438\n",
      "[35]\tvalidation_0-mlogloss:0.83202\n",
      "[36]\tvalidation_0-mlogloss:0.83030\n",
      "[37]\tvalidation_0-mlogloss:0.82871\n",
      "[38]\tvalidation_0-mlogloss:0.82677\n",
      "[39]\tvalidation_0-mlogloss:0.82466\n",
      "[40]\tvalidation_0-mlogloss:0.82378\n",
      "[41]\tvalidation_0-mlogloss:0.82235\n",
      "[42]\tvalidation_0-mlogloss:0.82095\n",
      "[43]\tvalidation_0-mlogloss:0.81936\n",
      "[44]\tvalidation_0-mlogloss:0.81825\n",
      "[45]\tvalidation_0-mlogloss:0.81725\n",
      "[46]\tvalidation_0-mlogloss:0.81702\n",
      "[47]\tvalidation_0-mlogloss:0.81604\n",
      "[48]\tvalidation_0-mlogloss:0.81515\n",
      "[49]\tvalidation_0-mlogloss:0.81380\n",
      "[50]\tvalidation_0-mlogloss:0.81322\n",
      "[51]\tvalidation_0-mlogloss:0.81226\n",
      "[52]\tvalidation_0-mlogloss:0.81087\n",
      "[53]\tvalidation_0-mlogloss:0.81022\n",
      "[54]\tvalidation_0-mlogloss:0.81009\n",
      "[55]\tvalidation_0-mlogloss:0.80907\n",
      "[56]\tvalidation_0-mlogloss:0.80849\n",
      "[57]\tvalidation_0-mlogloss:0.80797\n",
      "[58]\tvalidation_0-mlogloss:0.80772\n",
      "[59]\tvalidation_0-mlogloss:0.80666\n",
      "[60]\tvalidation_0-mlogloss:0.80575\n",
      "[61]\tvalidation_0-mlogloss:0.80493\n",
      "[62]\tvalidation_0-mlogloss:0.80395\n",
      "[63]\tvalidation_0-mlogloss:0.80340\n",
      "[64]\tvalidation_0-mlogloss:0.80252\n",
      "[65]\tvalidation_0-mlogloss:0.80208\n",
      "[66]\tvalidation_0-mlogloss:0.80124\n",
      "[67]\tvalidation_0-mlogloss:0.80092\n",
      "[68]\tvalidation_0-mlogloss:0.79992\n",
      "[69]\tvalidation_0-mlogloss:0.79978\n",
      "[70]\tvalidation_0-mlogloss:0.79941\n",
      "[71]\tvalidation_0-mlogloss:0.79940\n",
      "[72]\tvalidation_0-mlogloss:0.79953\n",
      "[73]\tvalidation_0-mlogloss:0.79919\n",
      "[74]\tvalidation_0-mlogloss:0.79838\n",
      "[75]\tvalidation_0-mlogloss:0.79813\n",
      "[76]\tvalidation_0-mlogloss:0.79755\n",
      "[77]\tvalidation_0-mlogloss:0.79753\n",
      "[78]\tvalidation_0-mlogloss:0.79635\n",
      "[79]\tvalidation_0-mlogloss:0.79533\n",
      "[80]\tvalidation_0-mlogloss:0.79496\n",
      "[81]\tvalidation_0-mlogloss:0.79451\n",
      "[82]\tvalidation_0-mlogloss:0.79471\n",
      "[83]\tvalidation_0-mlogloss:0.79416\n",
      "[84]\tvalidation_0-mlogloss:0.79369\n",
      "[85]\tvalidation_0-mlogloss:0.79396\n",
      "[86]\tvalidation_0-mlogloss:0.79360\n",
      "[87]\tvalidation_0-mlogloss:0.79293\n",
      "[88]\tvalidation_0-mlogloss:0.79241\n",
      "[89]\tvalidation_0-mlogloss:0.79195\n",
      "[90]\tvalidation_0-mlogloss:0.79153\n",
      "[91]\tvalidation_0-mlogloss:0.79085\n",
      "[92]\tvalidation_0-mlogloss:0.79062\n",
      "[93]\tvalidation_0-mlogloss:0.79051\n",
      "[94]\tvalidation_0-mlogloss:0.78938\n",
      "[95]\tvalidation_0-mlogloss:0.78942\n",
      "[96]\tvalidation_0-mlogloss:0.78929\n",
      "[97]\tvalidation_0-mlogloss:0.78915\n",
      "[98]\tvalidation_0-mlogloss:0.78926\n",
      "[99]\tvalidation_0-mlogloss:0.78904\n",
      "[100]\tvalidation_0-mlogloss:0.78859\n",
      "[101]\tvalidation_0-mlogloss:0.78834\n",
      "[102]\tvalidation_0-mlogloss:0.78799\n",
      "[103]\tvalidation_0-mlogloss:0.78795\n",
      "[104]\tvalidation_0-mlogloss:0.78765\n",
      "[105]\tvalidation_0-mlogloss:0.78786\n",
      "[106]\tvalidation_0-mlogloss:0.78780\n",
      "[107]\tvalidation_0-mlogloss:0.78771\n",
      "[108]\tvalidation_0-mlogloss:0.78753\n",
      "[109]\tvalidation_0-mlogloss:0.78739\n",
      "[110]\tvalidation_0-mlogloss:0.78707\n",
      "[111]\tvalidation_0-mlogloss:0.78669\n",
      "[112]\tvalidation_0-mlogloss:0.78673\n",
      "[113]\tvalidation_0-mlogloss:0.78601\n",
      "[114]\tvalidation_0-mlogloss:0.78560\n",
      "[115]\tvalidation_0-mlogloss:0.78556\n",
      "[116]\tvalidation_0-mlogloss:0.78561\n",
      "[117]\tvalidation_0-mlogloss:0.78510\n",
      "[118]\tvalidation_0-mlogloss:0.78478\n",
      "[119]\tvalidation_0-mlogloss:0.78462\n",
      "[120]\tvalidation_0-mlogloss:0.78441\n",
      "[121]\tvalidation_0-mlogloss:0.78397\n",
      "[122]\tvalidation_0-mlogloss:0.78379\n",
      "[123]\tvalidation_0-mlogloss:0.78364\n",
      "[124]\tvalidation_0-mlogloss:0.78344\n",
      "[125]\tvalidation_0-mlogloss:0.78339\n",
      "[126]\tvalidation_0-mlogloss:0.78358\n",
      "[127]\tvalidation_0-mlogloss:0.78370\n",
      "[128]\tvalidation_0-mlogloss:0.78349\n",
      "[129]\tvalidation_0-mlogloss:0.78348\n",
      "[130]\tvalidation_0-mlogloss:0.78378\n",
      "[131]\tvalidation_0-mlogloss:0.78358\n",
      "[132]\tvalidation_0-mlogloss:0.78307\n",
      "[133]\tvalidation_0-mlogloss:0.78278\n",
      "[134]\tvalidation_0-mlogloss:0.78302\n",
      "[135]\tvalidation_0-mlogloss:0.78282\n",
      "[136]\tvalidation_0-mlogloss:0.78331\n",
      "[137]\tvalidation_0-mlogloss:0.78341\n",
      "[138]\tvalidation_0-mlogloss:0.78335\n",
      "[139]\tvalidation_0-mlogloss:0.78306\n",
      "[140]\tvalidation_0-mlogloss:0.78282\n",
      "[141]\tvalidation_0-mlogloss:0.78308\n",
      "[142]\tvalidation_0-mlogloss:0.78273\n",
      "[143]\tvalidation_0-mlogloss:0.78239\n",
      "[144]\tvalidation_0-mlogloss:0.78202\n",
      "[145]\tvalidation_0-mlogloss:0.78162\n",
      "[146]\tvalidation_0-mlogloss:0.78161\n",
      "[147]\tvalidation_0-mlogloss:0.78164\n",
      "[148]\tvalidation_0-mlogloss:0.78149\n",
      "[149]\tvalidation_0-mlogloss:0.78177\n",
      "[150]\tvalidation_0-mlogloss:0.78149\n",
      "[151]\tvalidation_0-mlogloss:0.78140\n",
      "[152]\tvalidation_0-mlogloss:0.78116\n",
      "[153]\tvalidation_0-mlogloss:0.78081\n",
      "[154]\tvalidation_0-mlogloss:0.78110\n",
      "[155]\tvalidation_0-mlogloss:0.78116\n",
      "[156]\tvalidation_0-mlogloss:0.78120\n",
      "[157]\tvalidation_0-mlogloss:0.78135\n",
      "[158]\tvalidation_0-mlogloss:0.78137\n",
      "[159]\tvalidation_0-mlogloss:0.78122\n",
      "[160]\tvalidation_0-mlogloss:0.78102\n",
      "[161]\tvalidation_0-mlogloss:0.78116\n",
      "[162]\tvalidation_0-mlogloss:0.78112\n",
      "[163]\tvalidation_0-mlogloss:0.78128\n",
      "[164]\tvalidation_0-mlogloss:0.78085\n",
      "[165]\tvalidation_0-mlogloss:0.78116\n",
      "[166]\tvalidation_0-mlogloss:0.78114\n",
      "[167]\tvalidation_0-mlogloss:0.78100\n",
      "[168]\tvalidation_0-mlogloss:0.78059\n",
      "[169]\tvalidation_0-mlogloss:0.78103\n",
      "[170]\tvalidation_0-mlogloss:0.78091\n",
      "[171]\tvalidation_0-mlogloss:0.78079\n",
      "[172]\tvalidation_0-mlogloss:0.78097\n",
      "[173]\tvalidation_0-mlogloss:0.78120\n",
      "[174]\tvalidation_0-mlogloss:0.78111\n",
      "[175]\tvalidation_0-mlogloss:0.78105\n",
      "[176]\tvalidation_0-mlogloss:0.78097\n",
      "[177]\tvalidation_0-mlogloss:0.78105\n",
      "[178]\tvalidation_0-mlogloss:0.78139\n",
      "[179]\tvalidation_0-mlogloss:0.78125\n",
      "[180]\tvalidation_0-mlogloss:0.78117\n",
      "[181]\tvalidation_0-mlogloss:0.78106\n",
      "[182]\tvalidation_0-mlogloss:0.78143\n",
      "[183]\tvalidation_0-mlogloss:0.78128\n",
      "[184]\tvalidation_0-mlogloss:0.78145\n",
      "[185]\tvalidation_0-mlogloss:0.78111\n",
      "[186]\tvalidation_0-mlogloss:0.78083\n",
      "[187]\tvalidation_0-mlogloss:0.78081\n",
      "[188]\tvalidation_0-mlogloss:0.78078\n",
      "training end\n"
     ]
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    objective='multi:softmax', # Used for multiclass classification\n",
    "    num_class=4,               # 4 Ordinal Classes: Indicator, Ideation, Behavior, Attempt\n",
    "    n_jobs=-1,                 # Use all CPU cores\n",
    "    random_state=42,\n",
    "    early_stopping_rounds=20   # prevent overfitting\n",
    ")\n",
    "\n",
    "print(\"training start\")\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=True\n",
    ")\n",
    "print(\"training end\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284ffc2f-f596-42f4-8450-c3c66a31116b",
   "metadata": {},
   "source": [
    "---\n",
    "### **Evaluation**\n",
    "\n",
    "We evaluate the model using:\n",
    "1.  **Standard Accuracy:** General correctness\n",
    "2.  **Graded Metrics:** Specifically designed for ordinal suicide risk (Sawhney et al.), implemented in `src.utils`\n",
    "    * **Graded Precision (GP)**\n",
    "    * **Graded Recall (GR)**\n",
    "    * **Graded F1 (GF1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6389af70-8564-48eb-81f5-02a0e61dda92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Accuracy: 0.6631\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Indicator       0.63      0.71      0.67       305\n",
      "    Ideation       0.69      0.80      0.74       530\n",
      "    Behavior       0.61      0.25      0.36       135\n",
      "     Attempt       0.59      0.15      0.24        66\n",
      "\n",
      "    accuracy                           0.66      1036\n",
      "   macro avg       0.63      0.48      0.50      1036\n",
      "weighted avg       0.65      0.66      0.64      1036\n",
      "\n",
      "\n",
      "=== Graded Metrics ===\n",
      "Graded Precision: 0.8986\n",
      "Graded Recall:    0.7645\n",
      "Graded F1-Score:  0.8262\n"
     ]
    }
   ],
   "source": [
    "# Predict on test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "# 1. Standard Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nSimple Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 2. Detailed Classification Report\n",
    "target_names = ['Indicator', 'Ideation', 'Behavior', 'Attempt']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "# 3. Graded Metrics (Project Requirement)\n",
    "# Using the custom function from src/utils.py\n",
    "graded_metrics = compute_graded_metrics(y_test, y_pred)\n",
    "print(\"\\n=== Graded Metrics ===\")\n",
    "print(f\"Graded Precision: {graded_metrics['graded_precision']:.4f}\")\n",
    "print(f\"Graded Recall:    {graded_metrics['graded_recall']:.4f}\")\n",
    "print(f\"Graded F1-Score:  {graded_metrics['graded_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836bf357-c980-4f6e-8548-66f6a84ebb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP 3.11)",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
