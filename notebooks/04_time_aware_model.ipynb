{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4062b37c-c7b6-4a4a-88b7-b8e649e8eb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "src_path = os.path.join(parent_dir, 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "    \n",
    "from loss import OrdinalLoss\n",
    "from utils import compute_graded_metrics\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "050a9684-696d-4d08-89e4-2e8377eb60c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MAX_LEN = 512   # tokens\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_CLASSES = 4\n",
    "ALPHA_ORDINAL = 1.5\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8b0c8f3-5ac9-42a3-b21d-a33fce5d6e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 2)) (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 3)) (2.3.5)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 4)) (1.7.2)\n",
      "Requirement already satisfied: torch>=2.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 5)) (2.9.1)\n",
      "Requirement already satisfied: transformers>=4.36.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 6)) (4.36.2)\n",
      "Requirement already satisfied: xgboost>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 7)) (3.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 8)) (3.10.7)\n",
      "Requirement already satisfied: jupyter>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from -r ../requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from pandas>=2.0.0->-r ../requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from pandas>=2.0.0->-r ../requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from pandas>=2.0.0->-r ../requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from scikit-learn>=1.3.0->-r ../requirements.txt (line 4)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from scikit-learn>=1.3.0->-r ../requirements.txt (line 4)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from scikit-learn>=1.3.0->-r ../requirements.txt (line 4)) (3.6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from torch>=2.2.0->-r ../requirements.txt (line 5)) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from torch>=2.2.0->-r ../requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from torch>=2.2.0->-r ../requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from torch>=2.2.0->-r ../requirements.txt (line 5)) (3.6)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from torch>=2.2.0->-r ../requirements.txt (line 5)) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from torch>=2.2.0->-r ../requirements.txt (line 5)) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from transformers>=4.36.0->-r ../requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.36.0->-r ../requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from matplotlib>=3.8.0->-r ../requirements.txt (line 8)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from matplotlib>=3.8.0->-r ../requirements.txt (line 8)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from matplotlib>=3.8.0->-r ../requirements.txt (line 8)) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from matplotlib>=3.8.0->-r ../requirements.txt (line 8)) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from matplotlib>=3.8.0->-r ../requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from matplotlib>=3.8.0->-r ../requirements.txt (line 8)) (3.2.5)\n",
      "Requirement already satisfied: notebook in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter>=1.0.0->-r ../requirements.txt (line 9)) (7.5.0)\n",
      "Requirement already satisfied: jupyter-console in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter>=1.0.0->-r ../requirements.txt (line 9)) (6.6.3)\n",
      "Requirement already satisfied: nbconvert in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter>=1.0.0->-r ../requirements.txt (line 9)) (7.16.6)\n",
      "Requirement already satisfied: ipykernel in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter>=1.0.0->-r ../requirements.txt (line 9)) (7.1.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter>=1.0.0->-r ../requirements.txt (line 9)) (8.1.8)\n",
      "Requirement already satisfied: jupyterlab in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r ../requirements.txt (line 2)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.2.0->-r ../requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: appnope>=0.1.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.8.16)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (9.7.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.6.0)\n",
      "Requirement already satisfied: psutil>=5.7 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (7.0.0)\n",
      "Requirement already satisfied: pyzmq>=25 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (5.14.3)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.8.5)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.5.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.2.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from ipywidgets->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (3.0.16)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jinja2->torch>=2.2.0->-r ../requirements.txt (line 5)) (3.0.3)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.0.5)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.28.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.3.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.17.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.28.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.28.0)\n",
      "Requirement already satisfied: notebook-shim>=0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.2.4)\n",
      "Requirement already satisfied: setuptools>=41.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (80.9.0)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.12.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.25.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.16.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.5.3)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.23.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.9.0)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.17.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.12.1)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.25.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (25.1.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.28.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.30.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.0.0)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.1.1)\n",
      "Requirement already satisfied: fqdn in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.1.0)\n",
      "Requirement already satisfied: uri-template in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (25.10.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (4.14.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (6.3.0)\n",
      "Requirement already satisfied: defusedxml in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.5.1)\n",
      "Requirement already satisfied: webencodings in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from bleach[css]!=5.0.0->nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.4.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.21.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from requests->transformers>=4.36.0->-r ../requirements.txt (line 6)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from requests->transformers>=4.36.0->-r ../requirements.txt (line 6)) (2.5.0)\n",
      "Requirement already satisfied: lark>=1.2.2 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.23)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (2.8)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter>=1.0.0->-r ../requirements.txt (line 9)) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5787ac0b-2ecf-4286-890e-bf70951bb2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages (0.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4f7ddbe-e96c-4862-a83d-39b53c2c740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 11972\n",
      "Val size: 1605\n",
      "Test size: 1036\n",
      "      users                                               text  sentiment  \\\n",
      "0        24  If there is a god it’s not the one that you th...  Indicator   \n",
      "1        24  I just wish it was ok to end our lives if that...   Ideation   \n",
      "2        24  Sometimes suicide is a very appropriate reacti...   Ideation   \n",
      "3        24  I’m exhausted but I can’t fall asleep the ment...   Ideation   \n",
      "4        24  The pain just gets worse. How does it keep get...   Ideation   \n",
      "...     ...                                                ...        ...   \n",
      "1031   1235  I want to kill myself but I'm scared. And I do...   Ideation   \n",
      "1032   1253  Yeah, that is how it is sometimes. I'll be gon...   Behavior   \n",
      "1033   1253  haha, noone's going to see this but here goes ...   Behavior   \n",
      "1034   1259  Just venting I tried to kill myself about two ...    Attempt   \n",
      "1035   1261  I did it.. I made a reddit account. Purely to ...  Indicator   \n",
      "\n",
      "            time        timestamp_dt  label_ordinal  \n",
      "0     1602823395 2020-10-16 04:43:15              0  \n",
      "1     1602826506 2020-10-16 05:35:06              1  \n",
      "2     1602901712 2020-10-17 02:28:32              1  \n",
      "3     1602924825 2020-10-17 08:53:45              1  \n",
      "4     1602926864 2020-10-17 09:27:44              1  \n",
      "...          ...                 ...            ...  \n",
      "1031  1603197380 2020-10-20 12:36:20              1  \n",
      "1032  1608418348 2020-12-19 22:52:28              2  \n",
      "1033  1609926971 2021-01-06 09:56:11              2  \n",
      "1034  1603464843 2020-10-23 14:54:03              3  \n",
      "1035  1607733133 2020-12-12 00:32:13              0  \n",
      "\n",
      "[1036 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qp/cfpchbnx73v1pnmhgwclp3m40000gn/T/ipykernel_69520/4138350335.py:3: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  train_df = pickle.load(open(\"../data/processed/train.pkl\", \"rb\"))\n",
      "/var/folders/qp/cfpchbnx73v1pnmhgwclp3m40000gn/T/ipykernel_69520/4138350335.py:4: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  val_df   = pickle.load(open(\"../data/processed/val.pkl\", \"rb\"))\n",
      "/var/folders/qp/cfpchbnx73v1pnmhgwclp3m40000gn/T/ipykernel_69520/4138350335.py:5: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  test_df  = pickle.load(open(\"../data/processed/test.pkl\", \"rb\"))\n"
     ]
    }
   ],
   "source": [
    "# Load PKL datasets\n",
    "\n",
    "train_df = pickle.load(open(\"../data/processed/train.pkl\", \"rb\"))\n",
    "val_df   = pickle.load(open(\"../data/processed/val.pkl\", \"rb\"))\n",
    "test_df  = pickle.load(open(\"../data/processed/test.pkl\", \"rb\"))\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n",
    "print(\"Test size:\", len(test_df))\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63238a24-f2f4-455c-8e59-6e336d24800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      users                                               text  sentiment  \\\n",
      "0        24  If there is a god it’s not the one that you th...  Indicator   \n",
      "1        24  I just wish it was ok to end our lives if that...   Ideation   \n",
      "2        24  Sometimes suicide is a very appropriate reacti...   Ideation   \n",
      "3        24  I’m exhausted but I can’t fall asleep the ment...   Ideation   \n",
      "4        24  The pain just gets worse. How does it keep get...   Ideation   \n",
      "...     ...                                                ...        ...   \n",
      "1031   1235  I want to kill myself but I'm scared. And I do...   Ideation   \n",
      "1032   1253  Yeah, that is how it is sometimes. I'll be gon...   Behavior   \n",
      "1033   1253  haha, noone's going to see this but here goes ...   Behavior   \n",
      "1034   1259  Just venting I tried to kill myself about two ...    Attempt   \n",
      "1035   1261  I did it.. I made a reddit account. Purely to ...  Indicator   \n",
      "\n",
      "            time        timestamp_dt  label_ordinal  hour  day_of_week  \\\n",
      "0     1602823395 2020-10-16 04:43:15              0     4            4   \n",
      "1     1602826506 2020-10-16 05:35:06              1     5            4   \n",
      "2     1602901712 2020-10-17 02:28:32              1     2            5   \n",
      "3     1602924825 2020-10-17 08:53:45              1     8            5   \n",
      "4     1602926864 2020-10-17 09:27:44              1     9            5   \n",
      "...          ...                 ...            ...   ...          ...   \n",
      "1031  1603197380 2020-10-20 12:36:20              1    12            1   \n",
      "1032  1608418348 2020-12-19 22:52:28              2    22            5   \n",
      "1033  1609926971 2021-01-06 09:56:11              2     9            2   \n",
      "1034  1603464843 2020-10-23 14:54:03              3    14            4   \n",
      "1035  1607733133 2020-12-12 00:32:13              0     0            5   \n",
      "\n",
      "       time_gap  time_since_start  normalized_gap  \n",
      "0           0.0               0.0        0.000000  \n",
      "1        3111.0            3111.0        8.043021  \n",
      "2       75206.0           78317.0       11.228000  \n",
      "3       23113.0          101430.0       10.048194  \n",
      "4        2039.0          103469.0        7.620705  \n",
      "...         ...               ...             ...  \n",
      "1031        0.0               0.0        0.000000  \n",
      "1032        0.0               0.0        0.000000  \n",
      "1033  1508623.0         1508623.0       14.226709  \n",
      "1034        0.0               0.0        0.000000  \n",
      "1035        0.0               0.0        0.000000  \n",
      "\n",
      "[1036 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "def add_timestamp_features(df):\n",
    "\n",
    "    df[\"timestamp_dt\"] = pd.to_datetime(df[\"timestamp_dt\"])\n",
    "    df[\"hour\"] = df[\"timestamp_dt\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"timestamp_dt\"].dt.dayofweek\n",
    "\n",
    "    # Sort by user & time\n",
    "    df = df.sort_values([\"users\", \"timestamp_dt\"])\n",
    "\n",
    "    # === Generated by Gemini ===\n",
    "    # Time difference to previous post\n",
    "    df[\"time_gap\"] = df.groupby(\"users\")[\"timestamp_dt\"].diff().dt.total_seconds()\n",
    "    df[\"time_gap\"] = df[\"time_gap\"].fillna(0)\n",
    "\n",
    "    # Time since user's first post\n",
    "    df[\"time_since_start\"] = (\n",
    "        df[\"timestamp_dt\"] - df.groupby(\"users\")[\"timestamp_dt\"].transform(\"min\")\n",
    "    ).dt.total_seconds()\n",
    "    # === End of Gemini-generated block ===\n",
    "    \n",
    "    df[\"normalized_gap\"] = np.log1p(df[\"time_gap\"])  # log(1 + seconds)\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = add_timestamp_features(train_df)\n",
    "val_df   = add_timestamp_features(val_df)\n",
    "test_df  = add_timestamp_features(test_df)\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d99dff9-e65a-4a9a-9148-858b4d9f5842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      users                                               text  sentiment  \\\n",
      "0        24  If there is a god it’s not the one that you th...  Indicator   \n",
      "1        24  I just wish it was ok to end our lives if that...   Ideation   \n",
      "2        24  Sometimes suicide is a very appropriate reacti...   Ideation   \n",
      "3        24  I’m exhausted but I can’t fall asleep the ment...   Ideation   \n",
      "4        24  The pain just gets worse. How does it keep get...   Ideation   \n",
      "...     ...                                                ...        ...   \n",
      "1031   1235  I want to kill myself but I'm scared. And I do...   Ideation   \n",
      "1032   1253  Yeah, that is how it is sometimes. I'll be gon...   Behavior   \n",
      "1033   1253  haha, noone's going to see this but here goes ...   Behavior   \n",
      "1034   1259  Just venting I tried to kill myself about two ...    Attempt   \n",
      "1035   1261  I did it.. I made a reddit account. Purely to ...  Indicator   \n",
      "\n",
      "            time        timestamp_dt  label_ordinal  hour  day_of_week  \\\n",
      "0     1602823395 2020-10-16 04:43:15              0     4            4   \n",
      "1     1602826506 2020-10-16 05:35:06              1     5            4   \n",
      "2     1602901712 2020-10-17 02:28:32              1     2            5   \n",
      "3     1602924825 2020-10-17 08:53:45              1     8            5   \n",
      "4     1602926864 2020-10-17 09:27:44              1     9            5   \n",
      "...          ...                 ...            ...   ...          ...   \n",
      "1031  1603197380 2020-10-20 12:36:20              1    12            1   \n",
      "1032  1608418348 2020-12-19 22:52:28              2    22            5   \n",
      "1033  1609926971 2021-01-06 09:56:11              2     9            2   \n",
      "1034  1603464843 2020-10-23 14:54:03              3    14            4   \n",
      "1035  1607733133 2020-12-12 00:32:13              0     0            5   \n",
      "\n",
      "       time_gap  time_since_start  normalized_gap  \n",
      "0           0.0               0.0        0.000000  \n",
      "1        3111.0            3111.0        8.043021  \n",
      "2       75206.0           78317.0       11.228000  \n",
      "3       23113.0          101430.0       10.048194  \n",
      "4        2039.0          103469.0        7.620705  \n",
      "...         ...               ...             ...  \n",
      "1031        0.0               0.0        0.000000  \n",
      "1032        0.0               0.0        0.000000  \n",
      "1033  1508623.0         1508623.0       14.226709  \n",
      "1034        0.0               0.0        0.000000  \n",
      "1035        0.0               0.0        0.000000  \n",
      "\n",
      "[1036 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# DeBERTa Tokenization\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "\n",
    "train_encodings = tokenizer(train_df[\"text\"].tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "val_encodings   = tokenizer(val_df[\"text\"].tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "test_encodings  = tokenizer(test_df[\"text\"].tolist(), truncation=True, padding=\"max_length\", max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0633c945-624d-45c0-bc23-a0f4bd2633ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created with BATCH_SIZE= 8\n"
     ]
    }
   ],
   "source": [
    "class TimeAwareDataset(Dataset):\n",
    "    def __init__(self, encodings, df):\n",
    "        self.input_ids = encodings['input_ids']\n",
    "        self.attention_mask = encodings['attention_mask']\n",
    "        self.targets = torch.tensor(df[\"label_ordinal\"].values, dtype=torch.long)\n",
    "        \n",
    "        # Extract and prepare time features: hour, day of week, time gap, normalized gap\n",
    "        # Ensure the order is consistent with the model's expectations\n",
    "        time_feats = df[[\"hour\", \"day_of_week\", \"time_gap\", \"normalized_gap\"]].values\n",
    "        self.time_feats = torch.tensor(time_feats, dtype=torch.float32)\n",
    "\n",
    "    # Will be used when calling DataLoader   \n",
    "    def __len__(self): \n",
    "        return len(self.targets)\n",
    "\n",
    "    # Will be used when calling DataLoader\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'time_feats': self.time_feats[idx],\n",
    "            'targets': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Create Datasets & DataLoaders\n",
    "train_dataset = TimeAwareDataset(train_encodings, train_df)\n",
    "val_dataset = TimeAwareDataset(val_encodings, val_df)\n",
    "test_dataset = TimeAwareDataset(test_encodings, test_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created with BATCH_SIZE=\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f06c9302-b7f6-4207-b74f-af646669454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Model, Loss, and Optimizer initialized.\n"
     ]
    }
   ],
   "source": [
    "class TimeAwareOrdinalModel(nn.Module):\n",
    "    def __init__(self, model_name, time_feat_dim, num_classes, lstm_hidden_size=256, dropout_rate=0.3):\n",
    "        super(TimeAwareOrdinalModel, self).__init__()\n",
    "        \n",
    "        # 1. DeBERTa Pre-trained model\n",
    "        self.text_model = AutoModel.from_pretrained(model_name)\n",
    "        embed_dim = self.text_model.config.hidden_size # DeBERTa Base 默認是 768\n",
    "        \n",
    "        # 2. BiLSTM layer: Used to process DeBERTa sequence output\n",
    "        # Input size is 768 (DeBERTa hidden layer size)\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=lstm_hidden_size, # 256\n",
    "            num_layers=1,\n",
    "            bidirectional=True, # bi-direction\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # BiLSTM output dimension: 2 * lstm_hidden_size (because it is bidirectional)\n",
    "        bilstm_output_dim = lstm_hidden_size * 2\n",
    "        \n",
    "        # 3. Total input dimension of merged features\n",
    "        # Total input dimension = BiLSTM output + temporal features (4)\n",
    "        total_input_dim = bilstm_output_dim + time_feat_dim\n",
    "        \n",
    "        # 4. Classifier header\n",
    "        self.fc1 = nn.Linear(total_input_dim, lstm_hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_layer = nn.Linear(lstm_hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, time_feats):\n",
    "        \n",
    "        # 1. Get DeBERTa sequence output\n",
    "        # text_output.last_hidden_state shape: (B, MAX_LEN, embed_dim=768)\n",
    "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = text_output.last_hidden_state\n",
    "        \n",
    "        # 2. Passing to BiLSTM\n",
    "        # Inputting the sequence into BiLSTM\n",
    "        lstm_output, _ = self.bilstm(sequence_output)\n",
    "        \n",
    "        # 3. Extracting the output of a BiLSTM: Typically, we take the output of the last time step of the sequence (or the sequence after max pooling/average pooling).\n",
    "        # Here we take the output of the last time step (the last token) of the BiLSTM.\n",
    "        # lstm_output shape: (B, MAX_LEN, 2 * lstm_hidden_size)\n",
    "        lstm_pooled = lstm_output[:, 0, :] # Extract the BiLSTM output of the first token (similar to [CLS]) from the sequence.\n",
    "        \n",
    "        # 4. Combined features: [BiLSTM output, temporal features] \n",
    "        combined_features = torch.cat((lstm_pooled, time_feats), dim=1)\n",
    "        \n",
    "        # 5. Passed to classification layer\n",
    "        out = self.fc1(combined_features)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        logits = self.output_layer(out)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Instantiation of model and loss function\n",
    "TIME_FEAT_DIM = 4 # hour, day_of_week, time_gap, normalized_gap\n",
    "model = TimeAwareOrdinalModel(MODEL_NAME, TIME_FEAT_DIM, NUM_CLASSES).to(DEVICE)\n",
    "loss_fn = OrdinalLoss(alpha=ALPHA_ORDINAL, num_classes=NUM_CLASSES, device=DEVICE)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model, Loss, and Optimizer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f2f512f4-a7eb-4833-ac33-308baebc7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        time_feats = batch['time_feats'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask, time_feats)\n",
    "        \n",
    "        # OrdinalLoss returns a shape of (B,) which needs to be calculated using mean().\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.mean().backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.mean().item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d065234-2282-4b12-a976-4b7559c4cec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluation functions defined.\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            time_feats = batch['time_feats'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "            \n",
    "            logits = model(input_ids, attention_mask, time_feats)\n",
    "            \n",
    "            # Predicted category (index with the highest logit count)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_targets.extend(targets.cpu().tolist())\n",
    "\n",
    "            metrics = compute_graded_metrics(all_targets, all_preds)\n",
    "    return metrics\n",
    "\n",
    "print(\"Training and evaluation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5fd7b0b-8326-4421-b8c6-11142f24aed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 1/4 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75f8ddd8503494baaa5ae464554a60e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/1497 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 17.68 GiB, other allocations: 393.25 MiB, max allowed: 18.13 GiB). Tried to allocate 96.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m avg_train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m     11\u001b[39m val_metrics = evaluate(model, val_loader, DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, loss_fn, optimizer, device)\u001b[39m\n\u001b[32m      9\u001b[39m targets = batch[\u001b[33m'\u001b[39m\u001b[33mtargets\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     11\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_feats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# OrdinalLoss returns a shape of (B,) which needs to be calculated using mean().\u001b[39;00m\n\u001b[32m     15\u001b[39m loss = loss_fn(logits, targets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mTimeAwareOrdinalModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, time_feats)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask, time_feats):\n\u001b[32m     33\u001b[39m \n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# 1. Get DeBERTa sequence output\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;66;03m# text_output.last_hidden_state shape: (B, MAX_LEN, embed_dim=768)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     text_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     sequence_output = text_output.last_hidden_state\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# 2. Passing to BiLSTM\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Inputting the sequence into BiLSTM\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1070\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1060\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m   1062\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m   1063\u001b[39m     input_ids=input_ids,\n\u001b[32m   1064\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1067\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m   1068\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1077\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m   1079\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:514\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    504\u001b[39m     output_states = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    505\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    506\u001b[39m         next_kv,\n\u001b[32m   (...)\u001b[39m\u001b[32m    511\u001b[39m         output_attentions,\n\u001b[32m    512\u001b[39m     )\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     output_states = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    524\u001b[39m     output_states, att_m = output_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:362\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    354\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    355\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    360\u001b[39m     output_attentions=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    361\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     attention_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    371\u001b[39m         attention_output, att_matrix = attention_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:293\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    285\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    286\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    292\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     self_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    302\u001b[39m         self_output, att_matrix = self_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/py311/lib/python3.11/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:718\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    716\u001b[39m     scale_factor += \u001b[32m1\u001b[39m\n\u001b[32m    717\u001b[39m scale = torch.sqrt(torch.tensor(query_layer.size(-\u001b[32m1\u001b[39m), dtype=torch.float) * scale_factor)\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m attention_scores = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    720\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n",
      "\u001b[31mRuntimeError\u001b[39m: MPS backend out of memory (MPS allocated: 17.68 GiB, other allocations: 393.25 MiB, max allowed: 18.13 GiB). Tried to allocate 96.00 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "MODEL_SAVE_PATH = \"time_aware_deberta_ordinal_best.pt\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    \n",
    "    # Train\n",
    "    avg_train_loss = train_epoch(model, train_loader, loss_fn, optimizer, DEVICE)\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, DEVICE)\n",
    "    \n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}\")\n",
    "    print(f\"Validation Metrics: Graded Precision={val_metrics['graded_precision']}, Graded Recall={val_metrics['graded_recall']}, Graded F1={val_metrics['graded_f1']}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_metrics['graded_f1'] > best_f1:\n",
    "        best_f1 = val_metrics['graded_f1']\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(\"Model saved! New best F1.\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c62211-d58a-4225-ad35-4a5ed71be80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Final Test Set Evaluation ---\")\n",
    "\n",
    "# Load the best model\n",
    "try:\n",
    "    model.load_state_dict(torch.load(MODEL_SAVE_PATH, map_location=DEVICE))\n",
    "    print(f\"Loaded best model from {MODEL_SAVE_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: Best model file {MODEL_SAVE_PATH} not found. Using final epoch model.\")\n",
    "\n",
    "# Evaluate test set\n",
    "test_metrics = evaluate(model, test_loader, DEVICE)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"Graded Precision: {test_metrics['graded_precision']}\")\n",
    "print(f\"Graded Recall: {test_metrics['graded_recall']}\")\n",
    "print(f\"Graded F1 Score: {test_metrics['graded_f1']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py311 env)",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
