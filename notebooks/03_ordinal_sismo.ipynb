{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46417bd4-3f5b-4cfc-b255-82dc037c24e3",
   "metadata": {},
   "source": [
    "# Approach 3: Time-Aware Ordinal SISMO Model (DeBERTa + LSTM)\n",
    "\n",
    "This notebook implements the ordinal regression model based on the SISMO framework, extending\n",
    "a transformer backbone (DeBERTa-v3) with a BiLSTM and an ordinal loss designed for graded\n",
    "suicide risk detection.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Approach 3 introduces an ordinal deep learning model following the SISMO framework\n",
    "(Sawhney et al.), adapted to the RSD-15K dataset.\n",
    "\n",
    "Our model integrates:\n",
    "\n",
    "1. **DeBERTa-v3 Transformer Backbone**  \n",
    "   Captures contextual semantic information from posts.\n",
    "\n",
    "2. **BiLSTM Layer**  \n",
    "   Learns temporal dependencies across token sequences.\n",
    "\n",
    "3. **Ordinal Regression Loss (SISMO Loss)**  \n",
    "   Penalizes predictions proportionally to the distance between true and predicted risk level.\n",
    "\n",
    "4. **Class-Balanced Training**  \n",
    "   Weighting of loss to address class imbalance, especially for *Behavior* and *Attempt*.\n",
    "\n",
    "This approach enables a more structured understanding of suicide risk progression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05664584-46bc-49d1-b31f-9de018a3d870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME    : microsoft/deberta-v3-base\n",
      "MAX_LEN       : 512\n",
      "BATCH_SIZE    : 8\n",
      "EPOCHS        : 4\n",
      "LEARNING_RATE : 2e-05\n",
      "NUM_CLASSES   : 4\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"microsoft/deberta-v3-base\"   # Backbone model\n",
    "MAX_LEN = 512                                # Tokenization max length\n",
    "BATCH_SIZE = 8                               # Training batch size\n",
    "EPOCHS = 4                                    # Number of training epochs\n",
    "LEARNING_RATE = 2e-5                          # LR\n",
    "NUM_CLASSES = 4                               # (Indicator, Ideation, Behavior, Attempt)\n",
    "\n",
    "print(\"MODEL_NAME    :\", MODEL_NAME)\n",
    "print(\"MAX_LEN       :\", MAX_LEN)\n",
    "print(\"BATCH_SIZE    :\", BATCH_SIZE)\n",
    "print(\"EPOCHS        :\", EPOCHS)\n",
    "print(\"LEARNING_RATE :\", LEARNING_RATE)\n",
    "print(\"NUM_CLASSES   :\", NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b9d036-30da-4cf6-ba0e-1538a98ece64",
   "metadata": {},
   "source": [
    "## Training Setup\n",
    "\n",
    "- **Backbone**: DeBERTa-v3-base  \n",
    "- **Max sequence length**: 512  \n",
    "- **Batch size**: 32  \n",
    "- **Optimizer**: AdamW  \n",
    "- **Learning rate**: 1e-5  \n",
    "- **Epochs**: 4  \n",
    "- **Gradient Accumulation**: 4 steps  \n",
    "- **Scheduler**: Linear warmup (10%)  \n",
    "- **Device**: Apple MPS or CPU  \n",
    "\n",
    "### Class Weights  \n",
    "Because *Attempt* and *Behavior* are under-represented,\n",
    "we compute class-balanced weights:\n",
    "\n",
    "| Class     | Count | Weight |\n",
    "|-----------|--------|--------|\n",
    "| Indicator | 305 | … |\n",
    "| Ideation  | 530 | … |\n",
    "| Behavior  | 135 | ↑ heavier |\n",
    "| Attempt   | 66  | ↑↑ strongest |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47982008-fe2b-40d1-9eb7-2a9affb1f68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED_DATA_DIR: /Users/serenechien/Desktop/Suicide-Risk-Detection/data/processed\n",
      "Train, Val, Test size: 11972, 1605, 1036\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>users</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>time</th>\n",
       "      <th>timestamp_dt</th>\n",
       "      <th>label_ordinal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No one understands how much I desperately want...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>1648483701</td>\n",
       "      <td>2022-03-28 16:08:21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Today I never wanted to live to see 25. That m...</td>\n",
       "      <td>Behavior</td>\n",
       "      <td>1651130449</td>\n",
       "      <td>2022-04-28 07:20:49</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Suicidal thoughts at / because of school For s...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>1662712545</td>\n",
       "      <td>2022-09-09 08:35:45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I feel like the pain will never end Everyday f...</td>\n",
       "      <td>Ideation</td>\n",
       "      <td>1638628371</td>\n",
       "      <td>2021-12-04 14:32:51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Is there even a point to living if you're not ...</td>\n",
       "      <td>Indicator</td>\n",
       "      <td>1639749228</td>\n",
       "      <td>2021-12-17 13:53:48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   users                                               text  sentiment  \\\n",
       "0      1  No one understands how much I desperately want...   Ideation   \n",
       "1      2  Today I never wanted to live to see 25. That m...   Behavior   \n",
       "2      3  Suicidal thoughts at / because of school For s...   Ideation   \n",
       "3      4  I feel like the pain will never end Everyday f...   Ideation   \n",
       "4      4  Is there even a point to living if you're not ...  Indicator   \n",
       "\n",
       "         time        timestamp_dt  label_ordinal  \n",
       "0  1648483701 2022-03-28 16:08:21              1  \n",
       "1  1651130449 2022-04-28 07:20:49              2  \n",
       "2  1662712545 2022-09-09 08:35:45              1  \n",
       "3  1638628371 2021-12-04 14:32:51              1  \n",
       "4  1639749228 2021-12-17 13:53:48              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: MPS (Apple Silicon GPU)\n",
      "Effective batch size = 8 x 1 = 8\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 1: Imports & Load Processed Data =====\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from src.utils import compute_graded_metrics\n",
    "from src.loss import OrdinalLoss\n",
    "\n",
    "\n",
    "PROCESSED_DATA_DIR = os.path.join(parent_dir, 'data', 'processed')\n",
    "print(\"PROCESSED_DATA_DIR:\", PROCESSED_DATA_DIR)\n",
    "\n",
    "train_df = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'train.pkl'))\n",
    "val_df   = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'val.pkl'))\n",
    "test_df  = pd.read_pickle(os.path.join(PROCESSED_DATA_DIR, 'test.pkl'))\n",
    "\n",
    "print(f\"Train, Val, Test size: {len(train_df)}, {len(val_df)}, {len(test_df)}\")\n",
    "display(train_df.head())\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using device: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using device: CPU\")\n",
    "\n",
    "\n",
    "# ---- Micro-batch ----\n",
    "MICRO_BATCH_SIZE = 8                        \n",
    "ACCUM_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE \n",
    "\n",
    "print(f\"Effective batch size = {MICRO_BATCH_SIZE} x {ACCUM_STEPS} = {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81be96fe-4df6-4f4c-a592-226b54423078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch input_ids shape     : torch.Size([8, 512])\n",
      "Batch attention_mask shape: torch.Size([8, 512])\n",
      "Batch labels shape        : torch.Size([8])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/suicide/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:566: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: Tokenizer, Dataset & DataLoaders =====\n",
    "\n",
    "TEXT_COL = \"text\"\n",
    "LABEL_COL = \"label_ordinal\"\n",
    "\n",
    "label2id = {\n",
    "    \"Indicator\": 0,\n",
    "    \"Ideation\": 1,\n",
    "    \"Behavior\": 2,\n",
    "    \"Attempt\": 3,\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class RSDDataset(Dataset):\n",
    "    def __init__(self, df, text_col, label_col):\n",
    "        self.texts = df[text_col].tolist()\n",
    "        self.labels = df[label_col].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        enc = tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    RSDDataset(train_df, TEXT_COL, LABEL_COL),\n",
    "    batch_size=MICRO_BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    RSDDataset(val_df, TEXT_COL, LABEL_COL),\n",
    "    batch_size=MICRO_BATCH_SIZE * 2,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    RSDDataset(test_df, TEXT_COL, LABEL_COL),\n",
    "    batch_size=MICRO_BATCH_SIZE * 2,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch input_ids shape     :\", batch[\"input_ids\"].shape)\n",
    "print(\"Batch attention_mask shape:\", batch[\"attention_mask\"].shape)\n",
    "print(\"Batch labels shape        :\", batch[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449332d0-7b9e-4b3f-8ecb-9c994cc8690e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apple Silicon GPU (MPS) for training.\n",
      "===== Start Training SISMO Ordinal Model =====\n",
      " Input Sample Counts (Support Counts): [305.0, 530.0, 135.0, 66.0]\n",
      "Training Epochs: 5 | Total Steps: 1871\n",
      "===== Configuration Complete. Starting Training Loop =====\n",
      "\n",
      "Epoch 1/5\n",
      "  Step 40 | Loss=1.3631\n",
      "  Step 80 | Loss=1.2487\n",
      "  Step 120 | Loss=1.0366\n",
      "  Step 160 | Loss=0.9237\n",
      "  Step 200 | Loss=1.0187\n",
      "  Step 240 | Loss=1.3495\n",
      "  Step 280 | Loss=1.0075\n",
      "  Step 320 | Loss=1.0036\n",
      "  Step 360 | Loss=0.8924\n",
      "  Step 400 | Loss=1.2793\n",
      "  Step 440 | Loss=1.1134\n",
      "  Step 480 | Loss=1.2534\n",
      "  Step 520 | Loss=0.8785\n",
      "  Step 560 | Loss=0.9956\n",
      "  Step 600 | Loss=0.9980\n",
      "  Step 640 | Loss=0.8883\n",
      "  Step 680 | Loss=1.2450\n",
      "  Step 720 | Loss=0.8559\n",
      "  Step 760 | Loss=0.9544\n",
      "  Step 800 | Loss=1.1152\n",
      "  Step 840 | Loss=1.1128\n",
      "  Step 880 | Loss=1.2532\n",
      "  Step 920 | Loss=0.9780\n",
      "  Step 960 | Loss=0.9587\n",
      "  Step 1000 | Loss=1.1398\n",
      "  Step 1040 | Loss=0.9935\n",
      "  Step 1080 | Loss=1.3549\n",
      "  Step 1120 | Loss=1.3602\n",
      "  Step 1160 | Loss=1.1075\n",
      "  Step 1200 | Loss=1.1289\n",
      "  Step 1240 | Loss=0.9724\n",
      "  Step 1280 | Loss=1.4581\n",
      "  Step 1320 | Loss=1.1069\n",
      "  Step 1360 | Loss=1.1122\n",
      "  Step 1400 | Loss=1.2332\n",
      "  Step 1440 | Loss=0.9836\n",
      "  Step 1480 | Loss=0.8560\n",
      "[Epoch 1] train_loss=1.1304 | val_acc=0.5489 | GP=0.8791 | GR=0.6698 | GF1=0.7603\n",
      "\n",
      "Epoch 2/5\n",
      "  Step 40 | Loss=0.8411\n",
      "  Step 80 | Loss=1.2342\n",
      "  Step 120 | Loss=0.9508\n",
      "  Step 160 | Loss=1.1101\n",
      "  Step 200 | Loss=1.2231\n",
      "  Step 240 | Loss=0.8571\n",
      "  Step 280 | Loss=1.1340\n",
      "  Step 320 | Loss=1.0723\n",
      "  Step 360 | Loss=1.5964\n",
      "  Step 400 | Loss=1.0024\n",
      "  Step 440 | Loss=1.2267\n",
      "  Step 480 | Loss=1.0012\n",
      "  Step 520 | Loss=1.1250\n",
      "  Step 560 | Loss=0.9475\n",
      "  Step 600 | Loss=0.8841\n",
      "  Step 640 | Loss=0.9688\n",
      "  Step 680 | Loss=0.9642\n",
      "  Step 720 | Loss=0.9941\n",
      "  Step 760 | Loss=1.2389\n",
      "  Step 800 | Loss=0.9911\n",
      "  Step 840 | Loss=1.1093\n",
      "  Step 880 | Loss=1.0007\n",
      "  Step 920 | Loss=0.8140\n",
      "  Step 960 | Loss=1.3594\n",
      "  Step 1000 | Loss=1.1422\n",
      "  Step 1040 | Loss=1.0545\n",
      "  Step 1080 | Loss=1.1458\n",
      "  Step 1120 | Loss=1.0860\n",
      "  Step 1160 | Loss=0.9908\n",
      "  Step 1200 | Loss=0.8230\n",
      "  Step 1240 | Loss=1.3078\n",
      "  Step 1280 | Loss=1.0641\n",
      "  Step 1320 | Loss=0.8500\n",
      "  Step 1360 | Loss=1.4777\n",
      "  Step 1400 | Loss=0.8655\n",
      "  Step 1440 | Loss=1.0467\n",
      "  Step 1480 | Loss=1.2325\n",
      "[Epoch 2] train_loss=1.1167 | val_acc=0.5520 | GP=0.9016 | GR=0.6505 | GF1=0.7557\n",
      "\n",
      "Epoch 3/5\n",
      "  Step 40 | Loss=1.1017\n",
      "  Step 80 | Loss=0.9764\n",
      "  Step 120 | Loss=0.8370\n",
      "  Step 160 | Loss=0.9866\n",
      "  Step 200 | Loss=1.1367\n",
      "  Step 240 | Loss=1.0476\n",
      "  Step 280 | Loss=1.1876\n",
      "  Step 320 | Loss=1.2516\n",
      "  Step 360 | Loss=1.3814\n",
      "  Step 400 | Loss=1.1479\n",
      "  Step 440 | Loss=1.2100\n",
      "  Step 480 | Loss=0.9619\n",
      "  Step 520 | Loss=1.4055\n",
      "  Step 560 | Loss=1.2390\n",
      "  Step 600 | Loss=1.1835\n",
      "  Step 640 | Loss=0.9058\n",
      "  Step 680 | Loss=0.8372\n",
      "  Step 720 | Loss=0.9599\n",
      "  Step 760 | Loss=1.1465\n",
      "  Step 800 | Loss=0.8468\n",
      "  Step 840 | Loss=0.8470\n",
      "  Step 880 | Loss=1.0948\n",
      "  Step 920 | Loss=0.9704\n",
      "  Step 960 | Loss=1.0907\n",
      "  Step 1000 | Loss=1.4032\n",
      "  Step 1040 | Loss=0.8570\n",
      "  Step 1080 | Loss=1.3119\n",
      "  Step 1120 | Loss=1.2022\n",
      "  Step 1160 | Loss=1.1349\n",
      "  Step 1200 | Loss=0.9142\n",
      "  Step 1240 | Loss=1.2339\n",
      "  Step 1280 | Loss=1.1244\n",
      "  Step 1320 | Loss=0.8663\n",
      "  Step 1360 | Loss=1.1404\n",
      "  Step 1400 | Loss=0.8873\n",
      "  Step 1440 | Loss=0.9660\n",
      "  Step 1480 | Loss=1.4376\n",
      "[Epoch 3] train_loss=1.1143 | val_acc=0.5583 | GP=0.8903 | GR=0.6679 | GF1=0.7633\n",
      "\n",
      "Epoch 4/5\n",
      "  Step 40 | Loss=1.1763\n",
      "  Step 80 | Loss=0.9526\n",
      "  Step 120 | Loss=1.0862\n",
      "  Step 160 | Loss=0.9442\n",
      "  Step 200 | Loss=1.1231\n",
      "  Step 240 | Loss=0.9624\n",
      "  Step 280 | Loss=0.8576\n",
      "  Step 320 | Loss=0.8491\n",
      "  Step 360 | Loss=0.8953\n",
      "  Step 400 | Loss=0.8732\n",
      "  Step 440 | Loss=0.8171\n",
      "  Step 480 | Loss=0.9895\n",
      "  Step 520 | Loss=1.2703\n",
      "  Step 560 | Loss=0.9564\n",
      "  Step 600 | Loss=1.2085\n",
      "  Step 640 | Loss=1.5524\n",
      "  Step 680 | Loss=1.1139\n",
      "  Step 720 | Loss=1.0664\n",
      "  Step 760 | Loss=1.1560\n",
      "  Step 800 | Loss=1.2999\n",
      "  Step 840 | Loss=1.2207\n",
      "  Step 880 | Loss=1.4087\n",
      "  Step 920 | Loss=1.0403\n",
      "  Step 960 | Loss=0.8259\n",
      "  Step 1000 | Loss=0.9642\n",
      "  Step 1040 | Loss=1.2822\n",
      "  Step 1080 | Loss=1.1489\n",
      "  Step 1120 | Loss=1.1644\n",
      "  Step 1160 | Loss=0.8025\n",
      "  Step 1200 | Loss=0.8639\n",
      "  Step 1240 | Loss=1.0135\n",
      "  Step 1280 | Loss=0.9856\n",
      "  Step 1320 | Loss=1.2038\n",
      "  Step 1360 | Loss=1.3093\n",
      "  Step 1400 | Loss=1.3904\n",
      "  Step 1440 | Loss=0.9818\n",
      "  Step 1480 | Loss=0.8589\n",
      "[Epoch 4] train_loss=1.1133 | val_acc=0.5676 | GP=0.8704 | GR=0.6972 | GF1=0.7742\n",
      "\n",
      "Epoch 5/5\n",
      "  Step 40 | Loss=1.3456\n",
      "  Step 80 | Loss=0.9646\n",
      "  Step 120 | Loss=0.8750\n",
      "  Step 160 | Loss=0.8349\n",
      "  Step 200 | Loss=0.9535\n",
      "  Step 240 | Loss=0.9867\n",
      "  Step 280 | Loss=0.9693\n",
      "  Step 320 | Loss=1.4261\n",
      "  Step 360 | Loss=0.9753\n",
      "  Step 400 | Loss=1.4379\n",
      "  Step 440 | Loss=1.5376\n",
      "  Step 480 | Loss=0.9744\n",
      "  Step 520 | Loss=0.8018\n",
      "  Step 560 | Loss=0.8900\n",
      "  Step 600 | Loss=1.1603\n",
      "  Step 640 | Loss=1.0644\n",
      "  Step 680 | Loss=0.9252\n",
      "  Step 720 | Loss=0.9543\n",
      "  Step 760 | Loss=0.9773\n",
      "  Step 800 | Loss=1.1693\n",
      "  Step 840 | Loss=0.8726\n",
      "  Step 880 | Loss=0.8646\n",
      "  Step 920 | Loss=1.0734\n",
      "  Step 960 | Loss=1.0729\n",
      "  Step 1000 | Loss=1.0800\n",
      "  Step 1040 | Loss=1.2646\n",
      "  Step 1080 | Loss=1.1024\n",
      "  Step 1120 | Loss=0.8760\n",
      "  Step 1160 | Loss=1.2391\n",
      "  Step 1200 | Loss=1.1023\n",
      "  Step 1240 | Loss=1.2987\n",
      "  Step 1280 | Loss=0.8559\n",
      "  Step 1320 | Loss=0.9898\n",
      "  Step 1360 | Loss=1.4243\n",
      "  Step 1400 | Loss=0.9641\n",
      "  Step 1440 | Loss=1.1030\n",
      "  Step 1480 | Loss=1.2439\n",
      "[Epoch 5] train_loss=1.1119 | val_acc=0.5645 | GP=0.8829 | GR=0.6816 | GF1=0.7693\n",
      "\n",
      "Best Val Graded F1: 0.7742\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "import os\n",
    "\n",
    "NUM_CLASSES = 4               \n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "EPOCHS = 5                    \n",
    "ACCUM_STEPS = 4              \n",
    "\n",
    "# ---------------- Device ----------------\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using Apple Silicon GPU (MPS) for training.\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Falling back to CPU.\")\n",
    "    \n",
    "\n",
    "# ---------------- Ordinal Loss ----------------\n",
    "class OrdinalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Ordinal Regression Loss based on SISMO paper, with class weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, num_classes=4, class_weights=None, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "        if class_weights is not None:\n",
    "            if isinstance(class_weights, list):\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "            elif isinstance(class_weights, torch.Tensor):\n",
    "                class_weights = class_weights.to(device)\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "        cost_matrix = torch.zeros((num_classes, num_classes))\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                cost_matrix[i, j] = abs(i - j)\n",
    "        self.register_buffer(\"cost_matrix\", cost_matrix.to(device))\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        cost = self.cost_matrix[targets]\n",
    "\n",
    " \n",
    "        soft_targets = torch.softmax(-self.alpha * cost, dim=1)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        loss = -torch.sum(soft_targets * log_probs, dim=1)\n",
    "\n",
    "        if hasattr(self, \"class_weights\") and self.class_weights is not None:\n",
    "            weights_per_sample = self.class_weights[targets]\n",
    "            loss = loss * weights_per_sample\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ---------------- SISMO model ----------------\n",
    "class SISMOOrdinalModel(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "      \n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "       \n",
    "        \n",
    "        hidden = self.backbone.config.hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden,\n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(256 * 2, num_classes)\n",
    "     \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        seq_output = outputs.last_hidden_state\n",
    "\n",
    "        lstm_out, (h_n, _) = self.lstm(seq_output)\n",
    "\n",
    "        h_forward = h_n[-2]\n",
    "        h_backward = h_n[-1]\n",
    "        pooled = torch.cat([h_forward, h_backward], dim=-1)\n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        return logits\n",
    "\n",
    "\n",
    "# ---------------- Train / Eval function ----------------\n",
    "def train_one_epoch(model, data_loader, optimizer, criterion, device, scheduler, ACCUM_STEPS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    accum_counter = 0\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        raw_loss = criterion(\n",
    "            logits=model(input_ids, attention_mask),\n",
    "            targets=labels\n",
    "        )\n",
    "\n",
    "        loss = raw_loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        accum_counter += 1\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        total_loss += raw_loss.item() * bs\n",
    "        total_examples += bs\n",
    "\n",
    "        if accum_counter == ACCUM_STEPS:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            accum_counter = 0\n",
    "\n",
    "        if (step + 1) % (ACCUM_STEPS * 10) == 0:\n",
    "            print(f\"  Step {step+1} | Loss={raw_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    if accum_counter > 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device, compute_graded_metrics):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "\n",
    "    metrics = compute_graded_metrics(all_labels, all_preds)\n",
    "    gp = metrics[\"graded_precision\"]\n",
    "    gr = metrics[\"graded_recall\"]\n",
    "    gf1 = metrics[\"graded_f1\"]\n",
    "\n",
    "    acc = (torch.tensor(all_labels) == torch.tensor(all_preds)).float().mean().item()\n",
    "\n",
    "    return acc, gp, gr, gf1\n",
    "\n",
    "\n",
    "# ---------------- Training Config ----------------\n",
    "print(\"===== Start Training SISMO Ordinal Model =====\")\n",
    "best_val_gf1 = 0.0\n",
    "\n",
    "support_counts = torch.tensor([305, 530, 135, 66], dtype=torch.float32)\n",
    "print(f\" Input Sample Counts (Support Counts): {support_counts.cpu().tolist()}\")\n",
    "\n",
    "\n",
    "class_weights = torch.tensor([0.7, 0.7, 1.3, 1.7], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "model = SISMOOrdinalModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = OrdinalLoss(\n",
    "    alpha=1.2,                     \n",
    "    num_classes=NUM_CLASSES,\n",
    "    class_weights=class_weights,\n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "TOTAL_STEPS = len(train_loader) * EPOCHS / ACCUM_STEPS\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(TOTAL_STEPS * WARMUP_RATIO),\n",
    "    num_training_steps=int(TOTAL_STEPS)\n",
    ")\n",
    "\n",
    "print(f\"Training Epochs: {EPOCHS} | Total Steps: {int(TOTAL_STEPS)}\")\n",
    "print(\"===== Configuration Complete. Starting Training Loop =====\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    train_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        DEVICE,\n",
    "        scheduler,\n",
    "        ACCUM_STEPS\n",
    "    )\n",
    "\n",
    "    val_acc, val_gp, val_gr, val_gf1 = evaluate(model, val_loader, DEVICE, compute_graded_metrics)\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] \"\n",
    "          f\"train_loss={train_loss:.4f} | \"\n",
    "          f\"val_acc={val_acc:.4f} | \"\n",
    "          f\"GP={val_gp:.4f} | GR={val_gr:.4f} | GF1={val_gf1:.4f}\")\n",
    "\n",
    "    if val_gf1 > best_val_gf1:\n",
    "        best_val_gf1 = val_gf1\n",
    "\n",
    "print(\"\\nBest Val Graded F1:\", best_val_gf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e5d2d9-dde9-4c17-934f-d067229ad958",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "We evaluate the model using:\n",
    "\n",
    "## 1. Standard Accuracy\n",
    "Basic correctness of predictions.\n",
    "\n",
    "## 2. Classification Report\n",
    "Precision, recall, and F1 for each of the four ordinal labels.\n",
    "\n",
    "## 3. Graded Metrics (Required for Project)\n",
    "These metrics account for ordinal distance between labels:\n",
    "\n",
    "- **Graded Precision (GP)**\n",
    "- **Graded Recall (GR)**\n",
    "- **Graded F1 (GF1)**\n",
    "\n",
    "Strategy A (Frozen Backbone) — Bottleneck\n",
    "\n",
    "When the BERT base model parameters were frozen:\n",
    "\t•\tThe classifier only received generic, static BERT features.\n",
    "\t•\tThese features were insufficient to identify the rare Attempt class.\n",
    "\t•\tEven with interventions like class weighting or bias shifting,\n",
    "the model never learned Attempt → Recall = 0.\n",
    "\n",
    "\n",
    "2.2 Strategy B (Full Fine-Tuning) — Success\n",
    "\n",
    "The final adopted strategy, Full Fine-Tuning, succeeded because:\n",
    "\n",
    "1. Unlocking BERT Parameters\n",
    "Allowed the model to adjust millions of parameters to learn precise semantics for the Attempt class.\n",
    "\n",
    "2. Gradient Checkpointing\n",
    "This technique prevented OOM errors on M3 16GB RAM:\n",
    "\t•\tReduced memory usage by 15–30%.\n",
    "\t•\tSlower training, but stable.\n",
    "\n",
    "3. Final Performance\n",
    "Achieved GF1 ≈ 0.79, demonstrating:\n",
    "\t•\tSuccessful detection of the rare Attempt class\n",
    "\t•\tBalanced performance across ordinal distances\n",
    "\t•\tCompliance with SISMO evaluation standards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4b93b27-5d7a-4c86-8418-920cd9e8172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Accuracy: 0.5531\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Indicator       0.51      0.51      0.51       305\n",
      "    Ideation       0.58      0.77      0.66       530\n",
      "    Behavior       0.36      0.06      0.10       135\n",
      "     Attempt       0.00      0.00      0.00        66\n",
      "\n",
      "    accuracy                           0.55      1036\n",
      "   macro avg       0.36      0.34      0.32      1036\n",
      "weighted avg       0.49      0.55      0.50      1036\n",
      "\n",
      "\n",
      "=== Graded Metrics ===\n",
      "Graded Precision: 0.8504\n",
      "Graded Recall:    0.7027\n",
      "Graded F1-Score:  0.7695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/suicide/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/miniconda3/envs/suicide/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/opt/miniconda3/envs/suicide/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "model_to_eval = model  \n",
    "\n",
    "model_to_eval.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        logits_adj = logits.clone()\n",
    "\n",
    "        logits_adj[:, 2] += 0.2   \n",
    "        logits_adj[:, 3] += 0.4  \n",
    "\n",
    "        preds = torch.argmax(logits_adj, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "y_test = all_labels\n",
    "y_pred = all_preds\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nSimple Accuracy: {acc:.4f}\")\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "target_names = ['Indicator', 'Ideation', 'Behavior', 'Attempt']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "graded_metrics = compute_graded_metrics(y_test, y_pred)\n",
    "print(\"\\n=== Graded Metrics ===\")\n",
    "print(f\"Graded Precision: {graded_metrics['graded_precision']:.4f}\")\n",
    "print(f\"Graded Recall:    {graded_metrics['graded_recall']:.4f}\")\n",
    "print(f\"Graded F1-Score:  {graded_metrics['graded_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20bd0773-485e-4799-b25d-ce87b305ce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Silicon GPU (MPS)\n",
      "===== SISMO (Full Fine-Tuning) =====\n",
      " input (Support Counts): [305.0, 530.0, 135.0, 66.0]\n",
      " Log-Smoothed  (Class Weights): [0.9012120962142944, 0.8220493197441101, 1.049974799156189, 1.2267636060714722]\n",
      "Gradient Checkpointing\n",
      "Model Backbone UNFROZEN\n",
      "Epochs: 4 | steps: 1497\n",
      "===== Full Fine-Tuning Loop =====\n",
      "\n",
      "Epoch 1/4\n",
      "  Step 40 | Loss=1.2638\n",
      "  Step 80 | Loss=1.4319\n",
      "  Step 120 | Loss=1.1543\n",
      "  Step 160 | Loss=1.1092\n",
      "  Step 200 | Loss=1.0261\n",
      "  Step 240 | Loss=1.3535\n",
      "  Step 280 | Loss=1.1016\n",
      "  Step 320 | Loss=1.2285\n",
      "  Step 360 | Loss=1.3069\n",
      "  Step 400 | Loss=0.9568\n",
      "  Step 440 | Loss=1.0198\n",
      "  Step 480 | Loss=1.1157\n",
      "  Step 520 | Loss=1.1905\n",
      "  Step 560 | Loss=1.0742\n",
      "  Step 600 | Loss=0.9837\n",
      "  Step 640 | Loss=1.2951\n",
      "  Step 680 | Loss=0.9486\n",
      "  Step 720 | Loss=1.1452\n",
      "  Step 760 | Loss=1.1099\n",
      "  Step 800 | Loss=1.1205\n",
      "  Step 840 | Loss=1.1459\n",
      "  Step 880 | Loss=1.0300\n",
      "  Step 920 | Loss=1.1039\n",
      "  Step 960 | Loss=1.0704\n",
      "  Step 1000 | Loss=1.3050\n",
      "  Step 1040 | Loss=1.1404\n",
      "  Step 1080 | Loss=0.9140\n",
      "  Step 1120 | Loss=1.2387\n",
      "  Step 1160 | Loss=1.0367\n",
      "  Step 1200 | Loss=0.9871\n",
      "  Step 1240 | Loss=0.8766\n",
      "  Step 1280 | Loss=1.0060\n",
      "  Step 1320 | Loss=1.2795\n",
      "  Step 1360 | Loss=1.2063\n",
      "  Step 1400 | Loss=1.0450\n",
      "  Step 1440 | Loss=0.9390\n",
      "  Step 1480 | Loss=0.9521\n",
      "[Epoch 1] train_loss=1.1356 | val_acc=0.5701 | GP=0.8455 | GR=0.7246 | GF1=0.7804\n",
      "\n",
      "Epoch 2/4\n",
      "  Step 40 | Loss=1.1832\n",
      "  Step 80 | Loss=1.0116\n",
      "  Step 120 | Loss=0.9938\n",
      "  Step 160 | Loss=1.1294\n",
      "  Step 200 | Loss=0.8737\n",
      "  Step 240 | Loss=0.8467\n",
      "  Step 280 | Loss=0.9872\n",
      "  Step 320 | Loss=1.2288\n",
      "  Step 360 | Loss=1.1099\n",
      "  Step 400 | Loss=1.0517\n",
      "  Step 440 | Loss=0.9278\n",
      "  Step 480 | Loss=0.9593\n",
      "  Step 520 | Loss=1.0910\n",
      "  Step 560 | Loss=1.0459\n",
      "  Step 600 | Loss=0.8517\n",
      "  Step 640 | Loss=1.0154\n",
      "  Step 680 | Loss=0.8777\n",
      "  Step 720 | Loss=1.3157\n",
      "  Step 760 | Loss=1.4440\n",
      "  Step 800 | Loss=1.1748\n",
      "  Step 840 | Loss=1.4154\n",
      "  Step 880 | Loss=0.9201\n",
      "  Step 920 | Loss=1.0119\n",
      "  Step 960 | Loss=0.9690\n",
      "  Step 1000 | Loss=1.0703\n",
      "  Step 1040 | Loss=1.0587\n",
      "  Step 1080 | Loss=0.9484\n",
      "  Step 1120 | Loss=1.0803\n",
      "  Step 1160 | Loss=1.0641\n",
      "  Step 1200 | Loss=0.8029\n",
      "  Step 1240 | Loss=1.1801\n",
      "  Step 1280 | Loss=1.4048\n",
      "  Step 1320 | Loss=0.9235\n",
      "  Step 1360 | Loss=1.2800\n",
      "  Step 1400 | Loss=1.4023\n",
      "  Step 1440 | Loss=0.8624\n",
      "  Step 1480 | Loss=1.2721\n",
      "[Epoch 2] train_loss=1.0741 | val_acc=0.6137 | GP=0.8698 | GR=0.7439 | GF1=0.8019\n",
      "\n",
      "Epoch 3/4\n",
      "  Step 40 | Loss=1.2147\n",
      "  Step 80 | Loss=0.9153\n",
      "  Step 120 | Loss=1.0087\n",
      "  Step 160 | Loss=1.0531\n",
      "  Step 200 | Loss=0.9063\n",
      "  Step 240 | Loss=1.2244\n",
      "  Step 280 | Loss=0.8945\n",
      "  Step 320 | Loss=0.9493\n",
      "  Step 360 | Loss=1.1946\n",
      "  Step 400 | Loss=0.8066\n",
      "  Step 440 | Loss=1.0836\n",
      "  Step 480 | Loss=1.1657\n",
      "  Step 520 | Loss=0.8298\n",
      "  Step 560 | Loss=1.0058\n",
      "  Step 600 | Loss=1.1067\n",
      "  Step 640 | Loss=0.8346\n",
      "  Step 680 | Loss=0.9231\n",
      "  Step 720 | Loss=1.4128\n",
      "  Step 760 | Loss=0.9537\n",
      "  Step 800 | Loss=0.9955\n",
      "  Step 840 | Loss=0.9904\n",
      "  Step 880 | Loss=1.0395\n",
      "  Step 920 | Loss=0.9678\n",
      "  Step 960 | Loss=0.8872\n",
      "  Step 1000 | Loss=0.9905\n",
      "  Step 1040 | Loss=0.9522\n",
      "  Step 1080 | Loss=0.8972\n",
      "  Step 1120 | Loss=1.3027\n",
      "  Step 1160 | Loss=0.8502\n",
      "  Step 1200 | Loss=1.2846\n",
      "  Step 1240 | Loss=1.3444\n",
      "  Step 1280 | Loss=0.8285\n",
      "  Step 1320 | Loss=0.9338\n",
      "  Step 1360 | Loss=0.9317\n",
      "  Step 1400 | Loss=1.0743\n",
      "  Step 1440 | Loss=0.9368\n",
      "  Step 1480 | Loss=0.9910\n",
      "[Epoch 3] train_loss=1.0399 | val_acc=0.6100 | GP=0.9115 | GR=0.6984 | GF1=0.7909\n",
      "\n",
      "Epoch 4/4\n",
      "  Step 40 | Loss=1.0871\n",
      "  Step 80 | Loss=0.8561\n",
      "  Step 120 | Loss=0.9728\n",
      "  Step 160 | Loss=1.0496\n",
      "  Step 200 | Loss=1.0425\n",
      "  Step 240 | Loss=0.9704\n",
      "  Step 280 | Loss=0.7942\n",
      "  Step 320 | Loss=0.9717\n",
      "  Step 360 | Loss=1.0561\n",
      "  Step 400 | Loss=1.2303\n",
      "  Step 440 | Loss=0.8837\n",
      "  Step 480 | Loss=0.9749\n",
      "  Step 520 | Loss=0.8104\n",
      "  Step 560 | Loss=0.9088\n",
      "  Step 600 | Loss=0.9642\n",
      "  Step 640 | Loss=1.0915\n",
      "  Step 680 | Loss=0.9942\n",
      "  Step 720 | Loss=0.9602\n",
      "  Step 760 | Loss=0.7842\n",
      "  Step 800 | Loss=1.1337\n",
      "  Step 840 | Loss=1.0696\n",
      "  Step 880 | Loss=1.0594\n",
      "  Step 920 | Loss=0.9018\n",
      "  Step 960 | Loss=1.1020\n",
      "  Step 1000 | Loss=0.8112\n",
      "  Step 1040 | Loss=0.8820\n",
      "  Step 1080 | Loss=1.0268\n",
      "  Step 1120 | Loss=1.0726\n",
      "  Step 1160 | Loss=1.0035\n",
      "  Step 1200 | Loss=1.1563\n",
      "  Step 1240 | Loss=0.8225\n",
      "  Step 1280 | Loss=1.0155\n",
      "  Step 1320 | Loss=0.9566\n",
      "  Step 1360 | Loss=1.4774\n",
      "  Step 1400 | Loss=0.9729\n",
      "  Step 1440 | Loss=0.8909\n",
      "  Step 1480 | Loss=1.0894\n",
      "[Epoch 4] train_loss=1.0179 | val_acc=0.6081 | GP=0.8530 | GR=0.7551 | GF1=0.8011\n",
      "\\Best Graded F1: 0.8019\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, get_linear_schedule_with_warmup\n",
    "import os\n",
    "\n",
    "NUM_CLASSES = 4              \n",
    "MODEL_NAME = 'bert-base-uncased' \n",
    "EPOCHS = 4                    \n",
    "ACCUM_STEPS = 4              \n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"CPU \")\n",
    "# ===== BEGIN: GPT-5 -generated block =====\n",
    "class OrdinalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Ordinal Regression Loss based on SISMO paper, with class weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1.0, num_classes=4, class_weights=None, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "\n",
    "        if class_weights is not None:\n",
    "            if isinstance(class_weights, list):\n",
    "                class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "            elif isinstance(class_weights, torch.Tensor):\n",
    "                class_weights = class_weights.to(device)\n",
    "            self.register_buffer(\"class_weights\", class_weights)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "        cost_matrix = torch.zeros((num_classes, num_classes))\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                cost_matrix[i, j] = abs(i - j)\n",
    "        self.register_buffer(\"cost_matrix\", cost_matrix.to(device))\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "\n",
    "        cost = self.cost_matrix[targets]\n",
    "\n",
    "    \n",
    "        soft_targets = torch.softmax(-self.alpha * cost, dim=1)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)\n",
    "\n",
    "        loss = -torch.sum(soft_targets * log_probs, dim=1)\n",
    "\n",
    "        if hasattr(self, \"class_weights\") and self.class_weights is not None:\n",
    "            weights_per_sample = self.class_weights[targets]\n",
    "            loss = loss * weights_per_sample\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class SISMOOrdinalModel(nn.Module):\n",
    "    def __init__(self, num_classes=NUM_CLASSES):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "       \n",
    "        try:\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "            print(\"Gradient Checkpointing\")\n",
    "        except AttributeError:\n",
    "            print(\"Warning: Model does not support gradient_checkpointing_enable.\")\n",
    "        \n",
    "        print(\"Model Backbone UNFROZEN\")\n",
    "        \n",
    "        hidden = self.backbone.config.hidden_size\n",
    "\n",
    "        # LSTM head\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden,\n",
    "            hidden_size=256,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(256 * 2, num_classes)\n",
    "  \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        seq_output = outputs.last_hidden_state\n",
    "\n",
    "        lstm_out, (h_n, _) = self.lstm(seq_output)\n",
    "\n",
    "        h_forward = h_n[-2]\n",
    "        h_backward = h_n[-1]\n",
    "        pooled = torch.cat([h_forward, h_backward], dim=-1)\n",
    "\n",
    "        logits = self.classifier(self.dropout(pooled))\n",
    "        return logits\n",
    "\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, criterion, device, scheduler, ACCUM_STEPS):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    accum_counter = 0\n",
    "\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        raw_loss = criterion(\n",
    "            logits=model(input_ids, attention_mask),\n",
    "            targets=labels\n",
    "        )\n",
    "\n",
    "        loss = raw_loss / ACCUM_STEPS\n",
    "        loss.backward()\n",
    "        accum_counter += 1\n",
    "\n",
    "        bs = input_ids.size(0)\n",
    "        total_loss += raw_loss.item() * bs\n",
    "        total_examples += bs\n",
    "\n",
    "        if accum_counter == ACCUM_STEPS:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            accum_counter = 0\n",
    "\n",
    "        if (step + 1) % (ACCUM_STEPS * 10) == 0:\n",
    "            print(f\"  Step {step+1} | Loss={raw_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    if accum_counter > 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "    avg_loss = total_loss / total_examples\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device, compute_graded_metrics):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "\n",
    "    metrics = compute_graded_metrics(all_labels, all_preds)\n",
    "    gp = metrics[\"graded_precision\"]\n",
    "    gr = metrics[\"graded_recall\"]\n",
    "    gf1 = metrics[\"graded_f1\"]\n",
    "\n",
    "    acc = (torch.tensor(all_labels) == torch.tensor(all_preds)).float().mean().item()\n",
    "\n",
    "    return acc, gp, gr, gf1\n",
    "\n",
    "print(\"===== SISMO (Full Fine-Tuning) =====\")\n",
    "best_val_gf1 = 0.0\n",
    "\n",
    "support_counts = torch.tensor([305, 530, 135, 66], dtype=torch.float32)\n",
    "print(f\" input (Support Counts): {support_counts.cpu().tolist()}\")\n",
    "\n",
    "raw_weights = 1.0 / torch.log(support_counts + 1)\n",
    "class_weights = raw_weights / raw_weights.sum() * len(support_counts)\n",
    "class_weights = class_weights.to(DEVICE)\n",
    "\n",
    "print(f\" Log-Smoothed  (Class Weights): {class_weights.cpu().tolist()}\")\n",
    "\n",
    "\n",
    "model = SISMOOrdinalModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "criterion = OrdinalLoss(\n",
    "    alpha=1.5,                   \n",
    "    num_classes=NUM_CLASSES,\n",
    "    class_weights=class_weights, \n",
    "    device=DEVICE\n",
    ").to(DEVICE)\n",
    "\n",
    "LEARNING_RATE = 1e-5             \n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Note: train_loader, val_loader, and compute_graded_metrics must be defined elsewhere\n",
    "TOTAL_STEPS = len(train_loader) * EPOCHS / ACCUM_STEPS\n",
    "WARMUP_RATIO = 0.1\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(TOTAL_STEPS * WARMUP_RATIO),\n",
    "    num_training_steps=int(TOTAL_STEPS)\n",
    ")\n",
    "\n",
    "print(f\"Epochs: {EPOCHS} | steps: {int(TOTAL_STEPS)}\")\n",
    "print(\"===== Full Fine-Tuning Loop =====\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    train_loss = train_one_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        DEVICE,\n",
    "        scheduler,\n",
    "        ACCUM_STEPS\n",
    "    )\n",
    "\n",
    "    val_acc, val_gp, val_gr, val_gf1 = evaluate(model, val_loader, DEVICE, compute_graded_metrics)\n",
    "    \n",
    "    print(f\"[Epoch {epoch}] \"\n",
    "              f\"train_loss={train_loss:.4f} | \"\n",
    "              f\"val_acc={val_acc:.4f} | \"\n",
    "              f\"GP={val_gp:.4f} | GR={val_gr:.4f} | GF1={val_gf1:.4f}\")\n",
    "\n",
    "    if val_gf1 > best_val_gf1:\n",
    "        best_val_gf1 = val_gf1\n",
    "# ===== END: GPT-5 -generated block =====\n",
    "print(\"\\nBest Graded F1:\", best_val_gf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db604ae-ed7c-4c27-affc-1d6668838a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simple Accuracy: 0.5898\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Indicator       0.64      0.70      0.67       305\n",
      "    Ideation       0.69      0.63      0.66       530\n",
      "    Behavior       0.29      0.30      0.30       135\n",
      "     Attempt       0.31      0.38      0.34        66\n",
      "\n",
      "    accuracy                           0.59      1036\n",
      "   macro avg       0.48      0.50      0.49      1036\n",
      "weighted avg       0.60      0.59      0.59      1036\n",
      "\n",
      "\n",
      "=== Graded Metrics ===\n",
      "Graded Precision: 0.7944\n",
      "Graded Recall:    0.7954\n",
      "Graded F1-Score:  0.7949\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "model_to_eval = model  \n",
    "\n",
    "model_to_eval.eval()\n",
    "\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "\n",
    "        logits_adj = logits.clone()\n",
    "\n",
    "        logits_adj[:, 2] += 0.2   \n",
    "        logits_adj[:, 3] += 0.4  \n",
    "\n",
    "        preds = torch.argmax(logits_adj, dim=1)\n",
    "\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "\n",
    "y_test = all_labels\n",
    "y_pred = all_preds\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nSimple Accuracy: {acc:.4f}\")\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "target_names = ['Indicator', 'Ideation', 'Behavior', 'Attempt']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "graded_metrics = compute_graded_metrics(y_test, y_pred)\n",
    "print(\"\\n=== Graded Metrics ===\")\n",
    "print(f\"Graded Precision: {graded_metrics['graded_precision']:.4f}\")\n",
    "print(f\"Graded Recall:    {graded_metrics['graded_recall']:.4f}\")\n",
    "print(f\"Graded F1-Score:  {graded_metrics['graded_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b14ce-2d78-4a21-8b02-69a07561a129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
