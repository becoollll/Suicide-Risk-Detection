{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9a0175-c6f8-4d74-be7f-cbf3ee05b6d2",
   "metadata": {},
   "source": [
    "# **Approach 2: Deep Learning Baseline (DeBERTa)**\n",
    "\n",
    "In this notebook, we implement **Approach 2**, which utilizes a pre-trained Transformer model (**DeBERTa-v3**) fine-tuned on the RSD-15K dataset. Unlike the feature-based XGBoost baseline (Approach 1), this model learns contextual representations directly from raw text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac05c8c3-6d66-46a8-8866-47bb465a5344",
   "metadata": {},
   "source": [
    "---\n",
    "### **Import & Setup**\n",
    "We define the following hyperparameters based on standard practices for fine-tuning Transformers on consumer hardware (e.g., Apple Silicon M3 Pro):\n",
    "* **Model Architecture:** `microsoft/deberta-v3-base` (12 layers, 768 hidden size). We select the 'base' version to balance performance and computational efficiency.\n",
    "* **Max Sequence Length:** `512` tokens. This covers the majority of social media post lengths.\n",
    "* **Batch Size:** `32`. Optimized for 36GB+ Unified Memory.\n",
    "* **Learning Rate:** `2e-5`. A conservative learning rate to prevent catastrophic forgetting during fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889f0165-da2b-4d70-b30c-1f69beaeb1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophia/venvs/nlp311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import sys\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from src.utils import compute_graded_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a179c588-ced9-4c4b-98b8-7b05419b3c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\" \n",
    "MAX_LEN = 512 # limit is 512 tokens\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Detect MPS for Mac\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU)\")\n",
    "\n",
    "DATA_DIR = '../data/processed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382e08a-f411-4b28-bb3d-710dfcb7f2d7",
   "metadata": {},
   "source": [
    "---\n",
    "### **Data Preparation: From Text to Tokens**\n",
    "1.  **Define a Custom Dataset Class (`SuicideDataset`):** This handles the tokenization process, converting raw posts into numerical input IDs and attention masks using the **DeBERTa-v3 tokenizer**.\n",
    "2.  **Load Processed Data:** We load the 80/10/10 split data prepared in `00_preprocessing.ipynb`.\n",
    "3.  **Create DataLoaders:** These efficiently batch the data (Batch Size: 32) to feed into the GPU during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2485d4f0-a9ff-4f26-a35f-f11709e55d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sophia/venvs/nlp311/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "class SuicideDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # like a handbook for pytorch\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "\n",
    "        # Tokenize the text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "# Load Data\n",
    "train_df = pd.read_pickle(os.path.join(DATA_DIR, 'train.pkl'))\n",
    "val_df = pd.read_pickle(os.path.join(DATA_DIR, 'val.pkl'))\n",
    "test_df = pd.read_pickle(os.path.join(DATA_DIR, 'test.pkl'))\n",
    "\n",
    "# Initialize Tokenizer (DeBERTa-v3 uses SentencePiece)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "# Create DataLoaders -> spilt data with batch size we set\n",
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = SuicideDataset(\n",
    "        texts=df.text.to_numpy(),\n",
    "        labels=df.label_ordinal.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    # num_workers=0 is safer for MPS on Mac to avoid multiprocessing errors\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "train_loader = create_data_loader(train_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_loader = create_data_loader(val_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ba2a4-37b4-4577-85d8-c9de8b8e37bb",
   "metadata": {},
   "source": [
    "## **Model Initialization & Training Setup**\n",
    "\n",
    "1.  **Model Architecture:** We load the pre-trained `microsoft/deberta-v3-base` and add a classification head on top for our 4-class problem.\n",
    "2.  **Loss Function:** For Approach 2 (Deep Learning Baseline), we strictly adhere to the proposal by using **Standard Cross-Entropy Loss**. This treats the risk levels as independent categories, ignoring their ordinal nature (unlike Approach 3).\n",
    "3.  **Optimizer:** We use **AdamW** (Adaptive Moment Estimation with Weight Decay), the standard optimizer for Transformer models.\n",
    "4.  **Scheduler:** A linear learning rate scheduler with warmup is used to stabilize the early stages of fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83f56f4-baae-4683-b2f6-12660aa397e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "# --- 1. Initialize Model ---\n",
    "# Load DeBERTa-v3 with a classification head for 4 output classes\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_CLASSES)\n",
    "model = model.to(device) # Move model to MPS\n",
    "\n",
    "# --- 2. Optimization Setup ---\n",
    "# AdamW is generally preferred for Transformers over standard SGD\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Learning Rate Scheduler\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,         # No warmup steps needed for this dataset size\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# --- 3. Loss Function ---\n",
    "# standard Cross Entropy Loss\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# --- 4. Training Helper Functions ---\n",
    "\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    \"\"\"\n",
    "    Runs one full pass over the training data.\n",
    "    \"\"\"\n",
    "    model = model.train() # Set model to training mode\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    ## follow the batch size\n",
    "    for d in tqdm(data_loader, desc=\"Training Batch\"):\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"labels\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # DeBERTa outputs 'logits' (raw scores before Softmax)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate Loss\n",
    "        loss = loss_fn(logits, targets)\n",
    "        \n",
    "        # Calculate Accuracy for monitoring\n",
    "        _, preds = torch.max(logits, dim=1)\n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward pass (Gradient Descent)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping (Prevents \"exploding gradients\" in deep networks)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()   # Update weights\n",
    "        scheduler.step()   # Update learning rate\n",
    "        optimizer.zero_grad() # Reset gradients\n",
    "\n",
    "    return correct_predictions.float() / n_examples, np.mean(losses)\n",
    "\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    \"\"\"\n",
    "    Evaluates the model on validation/test data (No gradient updates).\n",
    "    \"\"\"\n",
    "    model = model.eval() # Set model to evaluation mode\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for speed\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fn(logits, targets)\n",
    "\n",
    "            _, preds = torch.max(logits, dim=1)\n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "    return correct_predictions.float() / n_examples, np.mean(losses)\n",
    "\n",
    "# ===== END: Gemini-generated block ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2ede03-f7af-40c5-8e68-27cb35ed38c6",
   "metadata": {},
   "source": [
    "## **Model Training**\n",
    "\n",
    "iterating through the dataset for a fixed number of **4 epochs**\n",
    "\n",
    "1.  **Training & Validation:** For each epoch, the model updates its weights on the training set and then evaluates its performance on the validation set.\n",
    "2.  **Monitoring:** We track loss and accuracy history to visualize learning progress.\n",
    "3.  **Model Checkpointing (Early Saving):** Instead of simply using the final model (which might be overfitted), we automatically save the model state (`best_deberta_model.bin`) whenever it achieves a new high score in **Validation Accuracy**. This ensures we use the most generalizable version of the model for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7688e737-7c9b-4c5d-8179-8379b17e11b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|█████████████████████████| 749/749 [45:45<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.7902 accuracy 0.6896\n",
      "Val   loss 0.6232 accuracy 0.7514\n",
      "--> Best Model Saved\n",
      "\n",
      "Epoch 2/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|███████████████████████| 749/749 [1:01:05<00:00,  4.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.5654 accuracy 0.7836\n",
      "Val   loss 0.6293 accuracy 0.7389\n",
      "\n",
      "Epoch 3/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|███████████████████████| 749/749 [1:00:02<00:00,  4.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.4403 accuracy 0.8380\n",
      "Val   loss 0.6744 accuracy 0.7508\n",
      "\n",
      "Epoch 4/4\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Batch: 100%|█████████████████████████| 749/749 [59:18<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss 0.3484 accuracy 0.8791\n",
      "Val   loss 0.7512 accuracy 0.7477\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "# Store training history for plotting\n",
    "history = {'train_acc': [], 'train_loss': [], 'val_acc': [], 'val_loss': []}\n",
    "\n",
    "# Initialize best accuracy to save the best model\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # --- Training Step ---\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model, \n",
    "        train_loader, \n",
    "        loss_fn, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler, \n",
    "        len(train_df)\n",
    "    )\n",
    "    print(f'Train loss {train_loss:.4f} accuracy {train_acc:.4f}')\n",
    "\n",
    "    # --- Validation Step ---\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model, \n",
    "        val_loader, \n",
    "        loss_fn, \n",
    "        device, \n",
    "        len(val_df)\n",
    "    )\n",
    "    print(f'Val   loss {val_loss:.4f} accuracy {val_acc:.4f}')\n",
    "\n",
    "    # --- History Tracking ---\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    # --- Model Checkpointing ---\n",
    "    # Save the model only if validation accuracy improves\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), '../models/best_deberta_model.bin')\n",
    "        best_accuracy = val_acc\n",
    "        print(\"--> Best Model Saved\")\n",
    "        \n",
    "    print() # Empty line for readability\n",
    "\n",
    "# ===== END: Gemini-generated block ====="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308f0bd4-1358-4989-9552-a95fb4e84bb9",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118101f1-3e5c-44fc-98ca-decdb866465c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Predicting on Test Set ---\n",
      "\n",
      "Simple Accuracy: 0.6969\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Indicator       0.73      0.71      0.72       305\n",
      "    Ideation       0.73      0.77      0.75       530\n",
      "    Behavior       0.54      0.47      0.50       135\n",
      "     Attempt       0.51      0.53      0.52        66\n",
      "\n",
      "    accuracy                           0.70      1036\n",
      "   macro avg       0.63      0.62      0.62      1036\n",
      "weighted avg       0.69      0.70      0.69      1036\n",
      "\n",
      "\n",
      "=== Graded Metrics (Approach 2) ===\n",
      "Graded Precision: 0.8562\n",
      "Graded Recall:    0.8407\n",
      "Graded F1-Score:  0.8484\n"
     ]
    }
   ],
   "source": [
    "# ===== BEGIN: Gemini-generated block =====\n",
    "\n",
    "# Load the best saved model\n",
    "model.load_state_dict(torch.load('../models/best_deberta_model.bin'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"--- Predicting on Test Set ---\")\n",
    "y_pred_list = []\n",
    "y_true_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for d in test_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        targets = d[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        \n",
    "        y_pred_list.extend(preds.cpu().numpy())\n",
    "        y_true_list.extend(targets.cpu().numpy())\n",
    "\n",
    "# ===== END: Gemini-generated block =====\n",
    "\n",
    "# 1. Standard Metrics\n",
    "acc = accuracy_score(y_true_list, y_pred_list)\n",
    "print(f\"\\nSimple Accuracy: {acc:.4f}\")\n",
    "\n",
    "# 2. Detailed Report\n",
    "target_names = ['Indicator', 'Ideation', 'Behavior', 'Attempt']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true_list, y_pred_list, target_names=target_names))\n",
    "\n",
    "# 3. Graded Metrics\n",
    "graded_metrics = compute_graded_metrics(y_true_list, y_pred_list)\n",
    "print(\"\\n=== Graded Metrics (Approach 2) ===\")\n",
    "print(f\"Graded Precision: {graded_metrics['graded_precision']:.4f}\")\n",
    "print(f\"Graded Recall:    {graded_metrics['graded_recall']:.4f}\")\n",
    "print(f\"Graded F1-Score:  {graded_metrics['graded_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4208dc-f5e3-4b08-afa9-b8095626f4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLP 3.11)",
   "language": "python",
   "name": "nlp311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
